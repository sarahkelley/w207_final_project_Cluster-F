{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outbrain Click Prediction Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Submitted by Jay Cordes, Sarah Kelly, Nicole Lee, and Lisa Minas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Background and Modeling Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outbrain is looking to predict which ad in a given display will be clicked by a user. They have provided an extensive amount of information about where an ad is displayed, what the ad is displaying, and user activity on multiple publisher sites in the United States between 14-June-2016 and 28-June-2016. Based on this information, we are asked to order by decending probability which ad within a display a user will click. \n",
    "\n",
    "We plan to approach this problem in a step-wise process. First, we will explore the data to better understand what information has been provided. Based on our initial investigation, we will transform the data to fit with the machine learning models we expect to be most appropriate for predicting clicks. If we find potential value in multiple models, we will quickly score a trained model on development data to determine which model we should focus on for optimization. Once a final model is chosen, we will further refine the model and judiciously score against the development model. Our final score will be determined by testing against a provided test dataset (clicks_test) and by the score judged via Kaggle.\n",
    "\n",
    "Note: We are running this notebook in Python3 because Kaggle uses Python3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load all of the datasets and save them to files we will not overwrite. These files are very large and take a while to download. To avoid re-downloading multiple times through development, we found it easiest to create copies of the original files that we could re-refer to when needing to adjust our transformations.\n",
    "\n",
    "Based on documentation, platform and traffic_source should be 1 of three values: 1, 2, 3. We discovered through EDA that these numbers came in as both ints and strs. To clean them up, we added a dtype statement to the read_csv below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# importing general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# importing ML libraries\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# importing visual analysis \n",
    "\n",
    "# # Other libraries from homework 2\n",
    "# SK-learn libraries for learning.\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# # SK-learn libraries for evaluation.\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # SK-learn libraries for feature extraction from text.\n",
    "# from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading all of the files\n",
    "clicks_train_og = pd.read_csv(\"./input/clicks_train.csv\")\n",
    "promoted_content_og = pd.read_csv(\"./input/promoted_content.csv\")\n",
    "doc_cats_og = pd.read_csv(\"./input/documents_categories.csv\")\n",
    "doc_ents_og = pd.read_csv(\"./input/documents_entities.csv\")\n",
    "doc_meta_og = pd.read_csv(\"./input/documents_meta.csv\")\n",
    "doc_topics_og = pd.read_csv(\"./input/documents_topics.csv\")\n",
    "events_og = pd.read_csv(\"./input/events.csv\", dtype={'platform': str, 'geo_location': str})\n",
    "page_views_og = pd.read_csv(\"./input/page_views_sample.csv\", dtype={'platform': str, 'traffic_source': str, 'geo_location': str})\n",
    "clicks_test_og = pd.read_csv(\"./input/clicks_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size has turned into a major issue with this project. Outbrain provided 9 CSVs of data averaging over 100 MB in compressed form with one file that is over 2 billion rows and 100GB uncompressed. This file is so large, Outbrain provided a 10th file which is a smaller sample to use for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Clicks_train size is: {}\".format(clicks_train_og.shape))\n",
    "print(\"Clicks_test size is: {}\".format(clicks_test_og.shape))\n",
    "print(\"Promoted Content size is: {}\".format(promoted_content_og.shape))\n",
    "print(\"Document Categories size is: {}\".format(doc_cats_og.shape))\n",
    "print(\"Document Entities size is: {}\".format(doc_ents_og.shape))\n",
    "print(\"Document Meta size is: {}\".format(doc_meta_og.shape))\n",
    "print(\"Document Topics size is: {}\".format(doc_topics_og.shape))\n",
    "print(\"Events size is: {}\".format(events_og.shape))\n",
    "print(\"Page Views size is: {}\".format(page_views_og.shape)) # note: full page views is ~100GB uncompressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore what we're training on: clicks_train and clicks test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Clicks_train:\")\n",
    "print(clicks_train_og.head())\n",
    "print(\"\\nClicks_test:\")\n",
    "print(clicks_test_og.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that test does not have the clicked label. To score our models as we build, we need to have a development set that we can predict and verify success against a known label. If we don't have a set-aside testing dataset, we'll likely overfit our model to the training data and have poorer results when scoring clicks_test and the hidden kaggle testind dataset. Therefore we will break the training data into training and development with a 70%-30% split. Otherwise, the two datasets look very similar. But display_id is the same for these first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clicks_train_og[clicks_train_og.display_id < 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few display_ids show us that there's multiple ad_ids per display_id. Within each display_id grouping, only one ad_id is which is marked as checked. Can a single ad_id be in multiple display_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clicks_train_og[clicks_train_og.ad_id == 250082]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, a single ad_id can appear in many different display_ids and each ad_id can be clicked on in multiple different display_ids. To learn more about any ad_id, Outbrain says to look into promoted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "promoted_content_og.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promoted content shows which campaign_id from which advertiser_id contains the ad_id. Each campaign_id can contain multiple ad_ids. Can an ad_id be related to multiple document_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Are there multiple rows per ad?: {}\".format(len(promoted_content_og['ad_id'].unique()) != len(promoted_content_og)))\n",
    "print(\"Is each document related to only one ad?: {}\".format(len(promoted_content_og['document_id'].unique()) == len(promoted_content_og)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a document? Outbrain says the document_X.csv files provide context on documents as well as Outbrain's confidence in each respective relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Document Categories:\")\n",
    "print(doc_cats_og.head())\n",
    "print(\"\\nDocument Entities:\")\n",
    "print(doc_ents_og.head())\n",
    "print(\"\\nDocument Topics:\")\n",
    "print(doc_topics_og.head())\n",
    "print(\"\\nDocument Meta:\")\n",
    "print(doc_meta_og.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each document_id can be described by a list of category_ids, entity_ids, and topic_ids. Apparently Outbrain either doesn't fully know what is in each document_id or they judge a document is mostly defined by one aspect but has influences of another. As an example: if a document talks about Brad Pitt and refers to his ex-wife as Angelina, we can say the document is 80% about Brad Pitt and 20% about Angelina. Or, alternatively, they are 80% sure the words _Brad Pitt_ refers to the person Brad Pitt and 20% sure the words _Angelina_ refers to the person Angelina Jole. Either interpretation will work for strength of relationship between a document aspect and the document.\n",
    "\n",
    "The meta file is in a different format than the other document files. This one shows specific details on the document. Based on the documentation on Kaggle, source_id is the full home address, publisher_id is the parent of source, and publish_time is when the document was launched on the source. So, for example, webpage with address _edition.cnn.com/20160623/some_news_article/index.html_ has a source_id=edition.cnn.com, a publisher_id=cnn.com, a publish_time=June 23 2016, and a document_id=some_news_article/index.html. These values are masked by numbers, but this example helps keep it straight for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've discussed what is refered to by an ad_id and a document_id, but where does the display_id and user come into this? Remember, our final output is to rank in decending order of click probability which ad_id will be clicked within the context of a specific **display_id**, not document_id. Outbrain says events covers display context. A display in clicks_train or clicks_test means one ad from the display was clicked. Therefore, events covers all events where at least one ad was clicked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "events_og.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('How many rows are there in events?: {:,}'.format(len(events_og)))\n",
    "print('How many users are there?: {:,}'.format(len(events_og.uuid.unique())))\n",
    "print('How many documents are there?: {:,}'.format(len(events_og.document_id.unique())))\n",
    "print('How many displays are there?: {:,}'.format(len(events_og.display_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seen = set()\n",
    "dup = set()\n",
    "for i in events_og.uuid:\n",
    "    if i in seen:\n",
    "        dup.add(i)\n",
    "    seen.add(i)\n",
    "events_og[events_og.uuid.isin(dup)].sort_values(by='uuid')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same user can be shown multiple different displays, but this happens rarely and display_id is unique per user_id * document_id. A display_id is associated with multiple documents which we interpret as a document can be pointed to by multiple displays. But since an ad does the actual pointing, an ad can appear in different displays and point to the same document. This is all pretty confusing so here's a diagram to help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='relationship_diagram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything interesting about the users in page_views?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "page_views_og.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that clicks are not mentioned here. This indicates page_views is about _any_ page view, not just the ones with a 'success' of a click. Hence the full page_views file is 100GB - there were a lot of documents viewed that never received a click."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we have, it's time to join the data. The relationships are complicated due to potential multiple joins and document_id having two different definitions depending on which table it comes from. Therefore, we join tables in a step-wise fashion, checking that the key field per tale is included as we build up our working dataset. Please see inline comments for what happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Documents are highest level item in the dataset.\n",
    "# To find the full set of documents covered by our datasets, \n",
    "# pull in the documents that are in both page_views and promoted_content.\n",
    "doc_ids = set(page_views_og['document_id']) & set(promoted_content_og['document_id'])\n",
    "\n",
    "# To only view displays and ads that are on our 'master' document list,\n",
    "# filter events to documents that are found in page_views and promoted_content.\n",
    "events = events_og[events_og['document_id'].isin(doc_ids)]\n",
    "\n",
    "# We only want to view ad clicks for displays that have document information.\n",
    "# So we filter clicks to displays found in events.\n",
    "clicks_train = clicks_train_og[clicks_train_og['display_id'].isin(events['display_id'])]\n",
    "\n",
    "# Because clicks may not cover all displays shown in events,\n",
    "# re-filter events to display_id's that are found in clicks_train.\n",
    "events = events_og[events_og['display_id'].isin(clicks_train['display_id'])]\n",
    "\n",
    "# By this point we have a solid list of ads, displays, and documents found in our activity files.\n",
    "# Now filter promoted content to ads found in the filtered training dataset for model fitting.\n",
    "promoted_content = promoted_content_og[promoted_content_og['ad_id'].isin(clicks_train['ad_id'])]\n",
    "\n",
    "# We only need document information about documents we are still considering after prior filters.\n",
    "# Filter document content files to only documents that have made it through to promoted_content for space reasons.\n",
    "doc_cats = doc_cats_og[doc_cats_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_ents = doc_ents_og[doc_ents_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_meta = doc_meta_og[doc_meta_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_topics = doc_topics_og[doc_topics_og['document_id'].isin(promoted_content['document_id'])]\n",
    "\n",
    "# Similarly, filter page views to documents found in events to make this file manageable.\n",
    "page_views = page_views_og[page_views_og['document_id'].isin(events['document_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our EDA, we are planning to fit models INSERT WHAT WE ARE DOING HERE AND WHY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>display_id</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>clicked</th>\n",
       "      <th>uuid</th>\n",
       "      <th>document_id_x</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>platform</th>\n",
       "      <th>geo_location</th>\n",
       "      <th>document_id_y</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_2002</th>\n",
       "      <th>cat_2003</th>\n",
       "      <th>cat_2004</th>\n",
       "      <th>cat_2005</th>\n",
       "      <th>cat_2006</th>\n",
       "      <th>cat_2100</th>\n",
       "      <th>advertiser_perc</th>\n",
       "      <th>campaign_perc</th>\n",
       "      <th>docx_view_freq</th>\n",
       "      <th>click_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>70153</td>\n",
       "      <td>0</td>\n",
       "      <td>d4f62cdcb39ad8</td>\n",
       "      <td>1779285</td>\n",
       "      <td>2687</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;WA&gt;819</td>\n",
       "      <td>933716</td>\n",
       "      <td>7516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046069</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>7701</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>149047</td>\n",
       "      <td>0</td>\n",
       "      <td>d4f62cdcb39ad8</td>\n",
       "      <td>1779285</td>\n",
       "      <td>2687</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;WA&gt;819</td>\n",
       "      <td>1169985</td>\n",
       "      <td>16636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>7701</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>169564</td>\n",
       "      <td>0</td>\n",
       "      <td>d4f62cdcb39ad8</td>\n",
       "      <td>1779285</td>\n",
       "      <td>2687</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;WA&gt;819</td>\n",
       "      <td>1394819</td>\n",
       "      <td>20109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7701</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>234713</td>\n",
       "      <td>1</td>\n",
       "      <td>d4f62cdcb39ad8</td>\n",
       "      <td>1779285</td>\n",
       "      <td>2687</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;WA&gt;819</td>\n",
       "      <td>1586431</td>\n",
       "      <td>245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271255</td>\n",
       "      <td>0.265217</td>\n",
       "      <td>7701</td>\n",
       "      <td>0.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>235443</td>\n",
       "      <td>0</td>\n",
       "      <td>d4f62cdcb39ad8</td>\n",
       "      <td>1779285</td>\n",
       "      <td>2687</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;WA&gt;819</td>\n",
       "      <td>1377696</td>\n",
       "      <td>11654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082631</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>7701</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   display_id   ad_id  clicked            uuid  document_id_x  timestamp  \\\n",
       "0          37   70153        0  d4f62cdcb39ad8        1779285       2687   \n",
       "1          37  149047        0  d4f62cdcb39ad8        1779285       2687   \n",
       "2          37  169564        0  d4f62cdcb39ad8        1779285       2687   \n",
       "3          37  234713        1  d4f62cdcb39ad8        1779285       2687   \n",
       "4          37  235443        0  d4f62cdcb39ad8        1779285       2687   \n",
       "\n",
       "  platform geo_location  document_id_y  campaign_id     ...      cat_2002  \\\n",
       "0        2    US>WA>819         933716         7516     ...           0.0   \n",
       "1        2    US>WA>819        1169985        16636     ...           0.0   \n",
       "2        2    US>WA>819        1394819        20109     ...           0.0   \n",
       "3        2    US>WA>819        1586431          245     ...           0.0   \n",
       "4        2    US>WA>819        1377696        11654     ...           0.0   \n",
       "\n",
       "   cat_2003  cat_2004  cat_2005  cat_2006  cat_2100  advertiser_perc  \\\n",
       "0       0.0       0.0       0.0       0.0       0.0         0.046069   \n",
       "1       0.0       0.0       0.0       0.0       0.0         0.137931   \n",
       "2       0.0       0.0       0.0       0.0       0.0         0.000000   \n",
       "3       0.0       0.0       0.0       0.0       0.0         0.271255   \n",
       "4       0.0       0.0       0.0       0.0       0.0         0.082631   \n",
       "\n",
       "   campaign_perc  docx_view_freq  click_perc  \n",
       "0       0.045455            7701    0.052632  \n",
       "1       0.137931            7701    0.076923  \n",
       "2       0.000000            7701    0.071429  \n",
       "3       0.265217            7701    0.281250  \n",
       "4       0.043478            7701    0.043478  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, format the training dataset\n",
    "\n",
    "def click_percent(dataset, ad_id, default_result, reg):\n",
    "    '''Returns the posterior probability of ad being clicked.\n",
    "    If ad has not been encountered before, assume mean click'''\n",
    "    \n",
    "    # count number of times ad has been seen\n",
    "    ad_total = len(dataset[dataset['ad_id'] == ad_id])\n",
    "    \n",
    "    # if ad has not been seen, returned the default_results\n",
    "    if ad_total == 0:\n",
    "        return default_result\n",
    "    # otherwise return percentage of times ad has been clicked, adjusted by a regularization term\n",
    "    else:\n",
    "        click_sum = np.sum(dataset[dataset['ad_id'] == ad_id].clicked) + 1.0\n",
    "        return click_sum / (ad_total + reg)\n",
    "\n",
    "# Merging information aout the displays to master dataset\n",
    "data = clicks_train.merge(events, on='display_id', how='left')\n",
    "# joins information about the display that the user saw\n",
    "# each display has a unique user id, doc id, and timestamp\n",
    "# events has the information about the display (who the user is, which site (document_id) it was on, when it was seen, from where, etc.)\n",
    "\n",
    "# Identifying which documents the ads refer to (aka destination documents)\n",
    "data = data.merge(promoted_content, on='ad_id', how='left')\n",
    "\n",
    "# Gather/bin data about the documents the ads refer to\n",
    "sparsetop = doc_topics.pivot(index='document_id', columns='topic_id', values='confidence_level')\n",
    "sparsetop.columns = ['top_' + str(col) for col in sparsetop.columns]\n",
    "\n",
    "sparsecat = doc_cats.pivot(index='document_id', columns='category_id', values='confidence_level')\n",
    "sparsecat.columns = ['cat_' + str(col) for col in sparsecat.columns]\n",
    "\n",
    "sparse = sparsetop.join(sparsecat, how='outer')\n",
    "sparse.fillna(0, inplace=True)\n",
    "del sparsetop, sparsecat\n",
    "\n",
    "sparse.reset_index(level=0, inplace=True)\n",
    "\n",
    "data = data.merge(sparse, left_on='document_id_y', right_on='document_id', how='left')\n",
    "\n",
    "# adding in advertiser information\n",
    "advr_success = dict(zip(data.advertiser_id.unique(),\n",
    "                        [sum(data[data['advertiser_id']==x]['clicked'])/len(data[data['advertiser_id']==x]) for x in data['advertiser_id'].unique()]))\n",
    "data['advertiser_perc'] = data['advertiser_id'].map(advr_success)\n",
    "\n",
    "# adding in campagin information\n",
    "camp_success = dict(zip(data.campaign_id.unique(), \n",
    "                        [sum(data[data['campaign_id']==x]['clicked'])/len(data[data['campaign_id']==x]) for x in data['campaign_id'].unique()]))\n",
    "data['campaign_perc'] = data['campaign_id'].map(camp_success)\n",
    "\n",
    "# adding in meta about document view frequencies\n",
    "doc_view_freq = dict(zip(page_views.document_id.unique(), \n",
    "                         [len(page_views[page_views.document_id==x]) for x in page_views.document_id.unique()]))\n",
    "data['docx_view_freq'] = data['document_id_x'].map(doc_view_freq)\n",
    "\n",
    "# Adding meta data about prior click percentage\n",
    "mean_click = np.mean(data[\"clicked\"])\n",
    "click_success = dict(zip(data.ad_id.unique(), \n",
    "                         [click_percent(data, x, mean_click, 10.0) for x in data[\"ad_id\"].unique()] ))\n",
    "data['click_perc'] = data['ad_id'].map(click_success)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting testing dataset\n",
    "-- This will return a memory error on local machine, do not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull from full events_og to make sure all display information is gathered\n",
    "data_test = clicks_test_og.merge(events_og, on='display_id', how='left')\n",
    "\n",
    "# find the ads from the entire promoted_content_og data and not the one filtered on clicks_train\n",
    "data_test = data_test.merge(promoted_content_og, on='ad_id', how='left')\n",
    "\n",
    "# matching top_X and cat_X categories from training to test. Sparse was created while developing the training dataset\n",
    "data_test = data_test.merge(sparse, left_on='document_id_y', right_on='document_id', how='left')\n",
    "print(data_test.display_id.unique().size)\n",
    "\n",
    "# Adding meta data from training\n",
    "data_test['docx_view_freq'] = data_test['document_id'].map(doc_view_freq)\n",
    "data_test['campaign_perc'] = data_test['campaign_id'].map(camp_success)\n",
    "data_test['advertiser_perc'] = data_test['advertiser_id'].map(advr_success)\n",
    "data_test['click_perc'] = data_test['ad_id'].map(click_success)\n",
    "\n",
    "# fill any nas so the modeling won't get wonky\n",
    "data_test.fillna(0, inplace=True)\n",
    "# check if there are any missing columns between the two datasets\n",
    "\n",
    "missing_col = set(data_test.columns) ^ set(data.columns)\n",
    "print(missing_col)\n",
    "\n",
    "# only one missing, clicked. But that's ok because we're going to take clicked out to make a training and development set for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training into train and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels length: 282205\n",
      "data length: (282205, 411)\n",
      "\n",
      "training label shape: (197543, 1)\n",
      "training data shape: (197543, 410)\n",
      "test label shape: (84662, 1)\n",
      "test data shape: (84662, 410)\n"
     ]
    }
   ],
   "source": [
    "def split_train_dev(dataset, train_percent = 0.7):\n",
    "    '''Splitting data into test and dev.\n",
    "    If train_precent is left at default, 70% will go into training and 30% into development.\n",
    "    If train_percent = 1, all data will go into training.\n",
    "    If train_percent = 0, all data will go into development.'''\n",
    "\n",
    "    labels = data['clicked']\n",
    "    labels = labels.values.reshape(-1,1)\n",
    "\n",
    "    print ('Labels length:', len(labels))\n",
    "    print ('data length:', data.shape)\n",
    "    print ('')\n",
    "        \n",
    "    train_data = data[:int(train_percent*len(data))].drop('clicked', 1)\n",
    "    dev_data = data[int(train_percent*len(data)):].drop('clicked', 1)\n",
    "\n",
    "    train_labels = labels[:int(train_percent*len(data))]\n",
    "    dev_labels = labels[int(train_percent*len(data)):]\n",
    "\n",
    "    print ('training label shape:', train_labels.shape)\n",
    "    print ('training data shape:', train_data.shape)\n",
    "    print ('test label shape:', dev_labels.shape)\n",
    "    print ('test data shape:', dev_data.shape)\n",
    "    \n",
    "    return train_data, train_labels, dev_data, dev_labels\n",
    "\n",
    "train_data, train_labels, dev_data, dev_labels = split_train_dev(data, train_percent = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del doc_cats, doc_meta, doc_ents, doc_topics\n",
    "del page_views, events, promoted_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMEANS (COPY AND PASTE BELOW 2 CELLS INTO FINAL VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1 :              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.84      0.85     72008\n",
      "          1       0.24      0.28      0.26     12654\n",
      "\n",
      "avg / total       0.77      0.76      0.76     84662\n",
      "\n",
      "k= 3 :              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.93      0.90     72008\n",
      "          1       0.33      0.19      0.24     12654\n",
      "\n",
      "avg / total       0.79      0.82      0.80     84662\n",
      "\n",
      "k= 5 :              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.95      0.91     72008\n",
      "          1       0.37      0.16      0.23     12654\n",
      "\n",
      "avg / total       0.79      0.83      0.80     84662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### INCLUDE THIS CELL\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data.fillna(0, inplace=True)\n",
    "dev_data.fillna(0, inplace=True)\n",
    "\n",
    "def kmeans(k_values):\n",
    "\n",
    "    for i in k_values:\n",
    "        neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "        neigh.fit(train_data[train_data.columns[11:]], train_labels)\n",
    "        preds = neigh.predict(dev_data[dev_data.columns[11:]])\n",
    "        print(\"k=\",i,\":\",classification_report(dev_labels, preds))\n",
    "\n",
    "k_values = [1, 3, 5]\n",
    "kmeans(k_values)\n",
    "\n",
    "# k=1 is overfitting to the training set completely, and yet the prediction accuracy is still very low.\n",
    "# the recall and precision of predicting an ad that was clicked (1) is extremeley low even with overfitting,\n",
    "# so with more generalization and higher k-values, we don't expect much better.\n",
    "\n",
    "# try again but only with the engineered variables of clicks, not including category/topic\n",
    "\n",
    "def kmeans2(k_values):\n",
    "\n",
    "    for i in k_values:\n",
    "        neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "        neigh.fit(train_data[train_data.columns[406:]], train_labels)\n",
    "        preds = neigh.predict(dev_data[dev_data.columns[406:]])\n",
    "        print(\"k=\",i,\":\",classification_report(dev_labels, preds))\n",
    "\n",
    "k_values = [1, 3, 5]\n",
    "kmeans2(k_values)\n",
    "\n",
    "# even running on the last 4 columns, feature engineered to look at overall averages of clicks based on \n",
    "# other features like advertiser, campaign do not result in good accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1393489, 197543]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f40bdc648810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mk_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mkmeans3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-f40bdc648810>\u001b[0m in \u001b[0;36mkmeans3\u001b[0;34m(k_values)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mneigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_kmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data_kmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"k=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicolelee/miniconda3/lib/python3.5/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \"\"\"\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicolelee/miniconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicolelee/miniconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1393489, 197543]"
     ]
    }
   ],
   "source": [
    "### INCLUDE THIS CELL\n",
    "\n",
    "# try binarizing platform & advertiser_id as more features for kmeans\n",
    "# kmeans finds similar neighbors, calculates dissimilarity distances between rows\n",
    "# so more differentiating factors can help kmeans find 'similar' neighbors for prediction\n",
    "# advertiser and platform may be good features that differentiate clicks\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_data.fillna(0, inplace=True)\n",
    "dev_data.fillna(0, inplace=True)\n",
    "\n",
    "#advertiser kills my kernel, so just leaving it with platform\n",
    "\n",
    "#advertiser = pd.get_dummies(train_data, columns={'advertiser_id'}, prefix=\"advertiser\")\n",
    "platform = pd.get_dummies(train_data, columns={'platform'}, prefix=\"platform\")\n",
    "train_data_kmeans = platform[platform.columns[:1].union(platform.columns[405:])]\n",
    "#train_data_kmeans = train_data_kmeans.merge(advertiser[advertiser.columns[:1].union(advertiser.columns[409:])], on='display_id')\n",
    "del train_data_kmeans[\"display_id\"]\n",
    "\n",
    "#advertiser_dev = pd.get_dummies(dev_data, columns={'advertiser_id'}, prefix=\"advertiser\")\n",
    "platform_dev = pd.get_dummies(dev_data, columns={'platform'}, prefix=\"platform\")\n",
    "dev_data_kmeans = platform_dev[platform_dev.columns[:1].union(platform_dev.columns[405:])]\n",
    "#dev_data_kmeans = dev_data_kmeans.merge(advertiser_dev[advertiser_dev.columns[:1].union(advertiser_dev.columns[409:])], on='display_id')\n",
    "del dev_data_kmeans[\"display_id\"]\n",
    "\n",
    "def kmeans3(k_values):\n",
    "\n",
    "    for i in k_values:\n",
    "        neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "        neigh.fit(train_data_kmeans, train_labels)\n",
    "        preds = neigh.predict(dev_data_kmeans)\n",
    "        print(\"k=\",i,\":\",classification_report(dev_labels, preds))\n",
    "\n",
    "k_values = [1, 3, 5]\n",
    "kmeans3(k_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have three datasets\n",
    "1. train_data, train_labels = 70% of clicks_train dataset. Used for training models.\n",
    "2. dev_data, dev_labels = remaining 30% of clicks_train dataset. Used for measuring accuracy of models on non-train data.\n",
    "3. test_data = clicks_test dataset for final scoring of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to print any of these datasets to csv, uncomment the appropriate line\n",
    "\n",
    "# data_test.to_csv('testingdata.cvs',index=False)\n",
    "\n",
    "# data.to_csv('fulltrainingdata.cvs',index=False)\n",
    "\n",
    "# train_data.to_csv('trainingdata.cvs',index=False)\n",
    "# train_labels.to_csv('traininglabels.cvs',index=False)\n",
    "\n",
    "# dev_data.to_csv('developmentdata.cvs',index=False)\n",
    "# dev_labels.to_csv('developmentlabels.cvs',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following Homework2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data_test.shape)\n",
    "print(clicks_test.display_id.unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running on real clicks_test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_data_full, train_labels_full, dev_data_full, dev_labels_full = split_train_dev(data, train_percent = .99)\n",
    "\n",
    "# fit a logistic regression on the numerical information from train_data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_data[train_data.columns[11:]].fillna(0), train_labels.reshape([1,-1]) )\n",
    "\n",
    "# # this line outputs predictions on the dev data\n",
    "# lr_click_prob = lr.predict_proba(dev_data[dev_data.columns[11:]].fillna(0))[:,1]\n",
    "# # scores\n",
    "# lr_score = lr.score(test_data[test_data.columns[5:]].fillna(0), test_labels)\n",
    "\n",
    "lr_click_prob = pd.DataFrame({'display_id': data_test['display_id'],\n",
    "                              'ad_id': data_test['ad_id'],\n",
    "                              'prediction': lr.predict_proba(data_test[data_test.columns[11:]].fillna(0))[:,1]\n",
    "                             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort predictions\n",
    "lr_click_prob.sort_values(by=['display_id','prediction'], ascending=False, inplace=True)\n",
    "\n",
    "# Format the data the way the submission requires\n",
    "output=lr_click_prob.groupby(['display_id'])['ad_id'].apply(lambda x:' '.join(map(str,x))).reset_index()\n",
    "\n",
    "# That's it for the simple solution (prior expectation)!\n",
    "output.to_csv('submission_20161211_1313.cvs',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix/Working space - this is stuff that hasn't been added to the 'full' run.\n",
    "### AKA Lisa's stuff that might go somewhere or might be dumped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lost comments\n",
    "** The below are all good comments that have become detached from their original location, probably from Lisa creating the dataload.py. Keeping them here for now so that they can be reattached as we work on formating the final notebook.**\n",
    "\n",
    "---\n",
    "\n",
    "Now we are merging information on what documents the ads referred to (from source: promoted_content).  \n",
    "In every display, there are multiple ads (within one document = document_id_x). Every ad refers to a different document, which is the site the ad is promoting (document_id_y). All the columns after document_id_y are information about that document (to which the ad is referring).\n",
    "\n",
    "---\n",
    "\n",
    "### Merge information about the documents the ads refer to\n",
    "All the doc files have information about the documents (websites) to which the ads refer to\n",
    "including confidence levels of which topics the ads referred to, which categories they're apart of, etc.\n",
    "\n",
    "We wanted to duplicate the idea of the CountVectorizer for the 'bag of words' model we used for spam detection, but since we're not counting words in a text, it's a little bit different. Since we have a 'dictionary' of categories and topics, we use that as our 'vocabulary.' Every document has a confidence level for one or more items in the vocabulary, so we create a sparse matrix with every topic and category as columns, and every document has a confidence level value in the respective columns. If they are not given a confidence level, we put 0 because the document most likely does not have anything to do with that category or topic (given the data provided by Outbrain).\n",
    "\n",
    "This data on the documents will help us separate ads from one another based on topic/category.  \n",
    "ie) why did ad A get clicked instead of ad B? We know ad A referred to document 1 whereas ad B referred to document 2, and now we have general information about the documents the ads referred to. We will merge this information in later steps.\n",
    "\n",
    "---\n",
    "\n",
    "creating dictionaries for % of ads clicked for every advertiser and campaign.  \n",
    "purpose: merge to master dataset as a feature for every ad, how often the advertiser and campaign are successful on average.\n",
    "\n",
    "---\n",
    "\n",
    "Add count of page views to every document that an ad appears in (document_id_x) as a feature, could tell us something about likelihood of ads being clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #################3 IGNORE THIS#############3\n",
    "\n",
    "# # events['country'], events['state'] = zip(*map(lambda x: str(x).split('>'), list(events['geo_location'])))\n",
    "\n",
    "# # #temp = map(lambda x: str(x).split('>'), list(page_views['geo_location']))\n",
    "# # print temp[:5]\n",
    "# # zip(*temp[:5]) # removes DMA\n",
    "\n",
    "# #events2 = pd.DataFrame(events['geo_location'].str.split(',').tolist(), columns = ['country', 'state', 'dma'])\n",
    "# events['geo_location'].str.split('>', expand= True)\n",
    "# geo = map(lambda x: str(x).split('>'), events['geo_location'])\n",
    "# zip(*geo)\n",
    "# country = [x[0] for x in geo]\n",
    "# #state = [x[1] for x in geo if x[1]]\n",
    "# if None:\n",
    "#     print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lr_prediction(train_data, train_labels, test_data, test_labels):\n",
    "    '''Returns the array of display_id, ad_id and probability it will be clicked'''\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_data[train_data.columns[11:]].fillna(0), train_labels)\n",
    "    lr_click_prob = lr.predict_proba(test_data[test_data.columns[11:]].fillna(0))[:,1]\n",
    "    lr_score = lr.score(test_data[test_data.columns[11:]].fillna(0), test_labels)\n",
    "    return pd.DataFrame({'display_id': test_data['display_id'],\n",
    "                         'ad_id': test_data['ad_id'],\n",
    "                         'prediction': lr_click_prob\n",
    "                        })\n",
    "lr_output = lr_prediction(train_data, train_labels, dev_data, dev_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
