{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Data Set Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Kaggle Script \"Making a mini-data set\" is run (FYI, it takes about 2 minutes to run) to reduce the size of the data to 40,000 instances, run this script to organize data into a single dataframe. \n",
    "\n",
    "Run this with the 8 csv files produced by the Kaggle Script in the same directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is a Python3 script because that is what Kaggle uses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things we've discovered:\n",
    "* Multiple ad_id per display_id\n",
    "* Multiple display_id per document_id\n",
    "* Ad_id can be in multiple display_id and multiple document_id\n",
    "* Only one ad_id per display_id is clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-91ad80e1e245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clicks_train_og = pd.read_csv(\"../input/clicks_train.csv\")\n",
    "promoted_content_og = pd.read_csv(\"../input/promoted_content.csv\")\n",
    "doc_cats_og = pd.read_csv(\"../input/documents_categories.csv\")\n",
    "doc_ents_og = pd.read_csv(\"../input/documents_entities.csv\")\n",
    "doc_meta_og = pd.read_csv(\"../input/documents_meta.csv\")\n",
    "doc_topics_og = pd.read_csv(\"../input/documents_topics.csv\")\n",
    "events_og = pd.read_csv(\"../input/events.csv\")\n",
    "page_views_og = pd.read_csv(\"../input/page_views_sample.csv\")\n",
    "clicks_test_og = pd.read_csv(\"../input/clicks_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_ids = set(page_views_og['document_id']) & set(promoted_content_og['document_id'])\n",
    "# pull in the content that is in both page_views and promoted_content\n",
    "\n",
    "events = events_og[events_og['document_id'].isin(doc_ids)]\n",
    "clicks_train = clicks_train_og[clicks_train_og['display_id'].isin(events['display_id'])]\n",
    "clicks_test = clicks_test_og[clicks_test_og['display_id'].isin(events['display_id'])]\n",
    "\n",
    "events = events[events['display_id'].isin(clicks_train['display_id'])]\n",
    "\n",
    "promoted_content = promoted_content_og[promoted_content_og['ad_id'].isin(clicks_train['ad_id'])]\n",
    "doc_cats = doc_cats_og[doc_cats_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_ents = doc_ents_og[doc_ents_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_meta = doc_meta_og[doc_meta_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_topics = doc_topics_og[doc_topics_og['document_id'].isin(promoted_content['document_id'])]\n",
    "page_views = page_views_og[page_views_og['document_id'].isin(events['document_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make master data merging all features to clicks_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge information about the displays to master dataset\n",
    "Events are only if the user CLICKED. This dataset will bring in information about the display_id's from events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def click_percent(ad_id, default_result, reg):\n",
    "    '''Returns the posterior probability of ad being clicked.\n",
    "    If ad has not been encountered before, assume mean click'''\n",
    "    \n",
    "    # count number of times ad has been seen\n",
    "    ad_total = len(dataset[dataset['ad_id'] == ad_id])\n",
    "    \n",
    "    # if ad has not been seen, returned the default_results\n",
    "    if ad_total == 0:\n",
    "        return default_result\n",
    "    # otherwise return percentage of times ad has been clicked, adjusted by a regularization term\n",
    "    else:\n",
    "        click_sum = np.sum(dataset[dataset['ad_id'] == ad_id].clicked) + 1.0\n",
    "        return click_sum / (ad_total + reg)\n",
    "    \n",
    "def format_data(dataset):\n",
    "\n",
    "    # Merging information aout the displays to master dataset\n",
    "    data = dataset.merge(events, on='display_id', how='left')\n",
    "    # joins information about the display that the user saw\n",
    "    # each display has a unique user id, doc id, and timestamp\n",
    "    # events has the information about the display (who the user is, which site (document_id) it was on, when it was seen, from where, etc.)\n",
    "\n",
    "    # Identifying which documents the ads refer to (aka destination documents)\n",
    "\n",
    "    data = data.merge(promoted_content, on='ad_id', how='left')\n",
    "\n",
    "    # Gather/bin data about the documents the ads refer to\n",
    "\n",
    "    sparsetop = doc_topics.pivot(index='document_id', \n",
    "                                 columns='topic_id', \n",
    "                                 values='confidence_level')\n",
    "    sparsetop.columns = ['top_' + str(col) for col in sparsetop.columns]\n",
    "\n",
    "    sparsecat = doc_cats.pivot(index='document_id', \n",
    "                               columns='category_id', \n",
    "                               values='confidence_level')\n",
    "    sparsecat.columns = ['cat_' + str(col) for col in sparsecat.columns]\n",
    "\n",
    "    sparse = sparsetop.join(sparsecat, how='outer')\n",
    "    sparse.fillna(0, inplace=True)\n",
    "\n",
    "    sparse.reset_index(level=0, inplace=True)\n",
    "\n",
    "    data = data.merge(sparse, \n",
    "                      left_on='document_id_y', \n",
    "                      right_on='document_id', \n",
    "                      how='left')\n",
    "    \n",
    "    # Adding meta data about the advertiser and campaign successes\n",
    "    if 'clicked' in clicks_train.columns:\n",
    "        advr_success = dict(zip(data.advertiser_id.unique(), \n",
    "                                [sum(data[data['advertiser_id']==x]['clicked'])/len(data[data['advertiser_id']==x]) for x in data['advertiser_id'].unique()]))\n",
    "        camp_success = dict(zip(data.campaign_id.unique(), \n",
    "                                [sum(data[data['campaign_id']==x]['clicked'])/len(data[data['campaign_id']==x]) for x in data['campaign_id'].unique()]))\n",
    "\n",
    "        data['campaign_perc'] = data['campaign_id'].map(camp_success)\n",
    "        data['advertiser_perc'] = data['advertiser_id'].map(advr_success)\n",
    "\n",
    "        doc_view_freq = dict(zip(page_views.document_id.unique(), [len(page_views[page_views.document_id==x]) for x in page_views.document_id.unique()]))\n",
    "        data['docx_view_freq'] = data['document_id_x'].map(doc_view_freq)\n",
    "\n",
    "        # Adding meta data about prior click percentage\n",
    "        mean_click = np.mean(dataset[\"clicked\"])\n",
    "        click_success = dict(zip(data.ad_id.unique(), [click_percent(x, mean_click, 10.0) for x in dataset[\"ad_id\"].unique()] ))\n",
    "        data['click_perc'] = data['ad_id'].map(click_success)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    \n",
    "data = format_data(clicks_train)\n",
    "#data_test = format_data(clicks_test) # doesn't work with click_test yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################3 IGNORE THIS#############3\n",
    "\n",
    "# Cleanup cells\n",
    "events['platform'] = map(str, events['platform'])\n",
    "page_views['platform'] = map(str, page_views['platform'])\n",
    "page_views['traffic_source'] = map(str, page_views['traffic_source'])\n",
    "\n",
    "# events['country'], events['state'] = zip(*map(lambda x: str(x).split('>'), list(events['geo_location'])))\n",
    "\n",
    "# #temp = map(lambda x: str(x).split('>'), list(page_views['geo_location']))\n",
    "# print temp[:5]\n",
    "# zip(*temp[:5]) # removes DMA\n",
    "\n",
    "#events2 = pd.DataFrame(events['geo_location'].str.split(',').tolist(), columns = ['country', 'state', 'dma'])\n",
    "events['geo_location'].str.split('>', expand= True)\n",
    "geo = map(lambda x: str(x).split('>'), events['geo_location'])\n",
    "zip(*geo)\n",
    "country = [x[0] for x in geo]\n",
    "#state = [x[1] for x in geo if x[1]]\n",
    "if None:\n",
    "    print('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are merging information on what documents the ads referred to (from source: promoted_content).  \n",
    "In every display, there are multiple ads (within one document = document_id_x). Every ad refers to a different document, which is the site the ad is promoting (document_id_y). All the columns after document_id_y are information about that document (to which the ad is referring)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge information about the documents the ads refer to\n",
    "All the doc files have information about the documents (websites) to which the ads refer to\n",
    "including confidence levels of which topics the ads referred to, which categories they're apart of, etc.\n",
    "\n",
    "We wanted to duplicate the idea of the CountVectorizer for the 'bag of words' model we used for spam detection, but since we're not counting words in a text, it's a little bit different. Since we have a 'dictionary' of categories and topics, we use that as our 'vocabulary.' Every document has a confidence level for one or more items in the vocabulary, so we create a sparse matrix with every topic and category as columns, and every document has a confidence level value in the respective columns. If they are not given a confidence level, we put 0 because the document most likely does not have anything to do with that category or topic (given the data provided by Outbrain).\n",
    "\n",
    "This data on the documents will help us separate ads from one another based on topic/category.  \n",
    "ie) why did ad A get clicked instead of ad B? We know ad A referred to document 1 whereas ad B referred to document 2, and now we have general information about the documents the ads referred to. We will merge this information in later steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating dictionaries for % of ads clicked for every advertiser and campaign.  \n",
    "purpose: merge to master dataset as a feature for every ad, how often the advertiser and campaign are successful on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add count of page views to every document that an ad appears in (document_id_x) as a feature, could tell us something about likelihood of ads being clicked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Make training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_labels(dataset, train_percent = False):\n",
    "    # Splitting dataset into data and labels\n",
    "\n",
    "    labels = data['clicked']\n",
    "    labels = labels.values.reshape(-1,1)\n",
    "    del data['clicked']\n",
    "\n",
    "    print 'Labels length:', len(labels)\n",
    "    print 'data length:', data.shape\n",
    "\n",
    "    # Making training and test set splits\n",
    "    # if not split defined, assume no split desired.\n",
    "    # split_percent = 1 means test results will be empty\n",
    "    if not split_percent:\n",
    "        split_percent = 1\n",
    "        \n",
    "    train_data = data[:int(split_percent*len(data))]\n",
    "    test_data = data[int(split_percentlen*(data)):]\n",
    "\n",
    "    train_labels = labels[:int(split_percent*len(data))]\n",
    "    test_labels = labels[int(split_percent*len(data)):]\n",
    "\n",
    "    print 'training label shape:', train_labels.shape\n",
    "    print 'training data shape:', train_data.shape\n",
    "    print 'test label shape:', test_labels.shape\n",
    "    print 'test data shape:', test_data\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = test_train_split(dataset, train_percent = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following Homework2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisam\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad_id</th>\n",
       "      <th>display_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197543</th>\n",
       "      <td>12392612</td>\n",
       "      <td>12392612</td>\n",
       "      <td>0.154566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197544</th>\n",
       "      <td>12392612</td>\n",
       "      <td>12392612</td>\n",
       "      <td>0.146541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197545</th>\n",
       "      <td>12392612</td>\n",
       "      <td>12392612</td>\n",
       "      <td>0.151116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197546</th>\n",
       "      <td>12392612</td>\n",
       "      <td>12392612</td>\n",
       "      <td>0.178808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197547</th>\n",
       "      <td>12392620</td>\n",
       "      <td>12392620</td>\n",
       "      <td>0.220336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197548</th>\n",
       "      <td>12392620</td>\n",
       "      <td>12392620</td>\n",
       "      <td>0.203909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197549</th>\n",
       "      <td>12392620</td>\n",
       "      <td>12392620</td>\n",
       "      <td>0.238116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197550</th>\n",
       "      <td>12392620</td>\n",
       "      <td>12392620</td>\n",
       "      <td>0.226653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197551</th>\n",
       "      <td>12392620</td>\n",
       "      <td>12392620</td>\n",
       "      <td>0.200199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197552</th>\n",
       "      <td>12392620</td>\n",
       "      <td>12392620</td>\n",
       "      <td>0.176542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197553</th>\n",
       "      <td>12393407</td>\n",
       "      <td>12393407</td>\n",
       "      <td>0.174452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197554</th>\n",
       "      <td>12393407</td>\n",
       "      <td>12393407</td>\n",
       "      <td>0.167625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197555</th>\n",
       "      <td>12393407</td>\n",
       "      <td>12393407</td>\n",
       "      <td>0.221674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197556</th>\n",
       "      <td>12393407</td>\n",
       "      <td>12393407</td>\n",
       "      <td>0.133773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197557</th>\n",
       "      <td>12393407</td>\n",
       "      <td>12393407</td>\n",
       "      <td>0.224631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197558</th>\n",
       "      <td>12393407</td>\n",
       "      <td>12393407</td>\n",
       "      <td>0.132604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197559</th>\n",
       "      <td>12393407</td>\n",
       "      <td>12393407</td>\n",
       "      <td>0.175955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197560</th>\n",
       "      <td>12393407</td>\n",
       "      <td>12393407</td>\n",
       "      <td>0.144002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197561</th>\n",
       "      <td>12393407</td>\n",
       "      <td>12393407</td>\n",
       "      <td>0.181854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197562</th>\n",
       "      <td>12393434</td>\n",
       "      <td>12393434</td>\n",
       "      <td>0.175156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197563</th>\n",
       "      <td>12393434</td>\n",
       "      <td>12393434</td>\n",
       "      <td>0.203909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197564</th>\n",
       "      <td>12393434</td>\n",
       "      <td>12393434</td>\n",
       "      <td>0.238116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197565</th>\n",
       "      <td>12393434</td>\n",
       "      <td>12393434</td>\n",
       "      <td>0.176959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197566</th>\n",
       "      <td>12393434</td>\n",
       "      <td>12393434</td>\n",
       "      <td>0.176542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197567</th>\n",
       "      <td>12393434</td>\n",
       "      <td>12393434</td>\n",
       "      <td>0.174757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197568</th>\n",
       "      <td>12394160</td>\n",
       "      <td>12394160</td>\n",
       "      <td>0.159889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197569</th>\n",
       "      <td>12394160</td>\n",
       "      <td>12394160</td>\n",
       "      <td>0.161158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197570</th>\n",
       "      <td>12394160</td>\n",
       "      <td>12394160</td>\n",
       "      <td>0.161788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197571</th>\n",
       "      <td>12394160</td>\n",
       "      <td>12394160</td>\n",
       "      <td>0.162451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197572</th>\n",
       "      <td>12394160</td>\n",
       "      <td>12394160</td>\n",
       "      <td>0.161730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282175</th>\n",
       "      <td>16872318</td>\n",
       "      <td>16872318</td>\n",
       "      <td>0.224827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282176</th>\n",
       "      <td>16872318</td>\n",
       "      <td>16872318</td>\n",
       "      <td>0.229304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282177</th>\n",
       "      <td>16872318</td>\n",
       "      <td>16872318</td>\n",
       "      <td>0.196075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282178</th>\n",
       "      <td>16872318</td>\n",
       "      <td>16872318</td>\n",
       "      <td>0.170995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282179</th>\n",
       "      <td>16872725</td>\n",
       "      <td>16872725</td>\n",
       "      <td>0.151234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282180</th>\n",
       "      <td>16872725</td>\n",
       "      <td>16872725</td>\n",
       "      <td>0.153546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282181</th>\n",
       "      <td>16872725</td>\n",
       "      <td>16872725</td>\n",
       "      <td>0.112192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282182</th>\n",
       "      <td>16872725</td>\n",
       "      <td>16872725</td>\n",
       "      <td>0.150847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282183</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.158565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282184</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.151251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282185</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.159101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282186</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.180469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282187</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.124899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282188</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.161633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282189</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.137283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282190</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.160618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282191</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.136593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282192</th>\n",
       "      <td>16873702</td>\n",
       "      <td>16873702</td>\n",
       "      <td>0.159850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282193</th>\n",
       "      <td>16873984</td>\n",
       "      <td>16873984</td>\n",
       "      <td>0.161903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282194</th>\n",
       "      <td>16873984</td>\n",
       "      <td>16873984</td>\n",
       "      <td>0.133837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282195</th>\n",
       "      <td>16873984</td>\n",
       "      <td>16873984</td>\n",
       "      <td>0.162191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282196</th>\n",
       "      <td>16873984</td>\n",
       "      <td>16873984</td>\n",
       "      <td>0.161808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282197</th>\n",
       "      <td>16873984</td>\n",
       "      <td>16873984</td>\n",
       "      <td>0.211302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282198</th>\n",
       "      <td>16873984</td>\n",
       "      <td>16873984</td>\n",
       "      <td>0.211302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282199</th>\n",
       "      <td>16874403</td>\n",
       "      <td>16874403</td>\n",
       "      <td>0.117902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282200</th>\n",
       "      <td>16874403</td>\n",
       "      <td>16874403</td>\n",
       "      <td>0.212334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282201</th>\n",
       "      <td>16874403</td>\n",
       "      <td>16874403</td>\n",
       "      <td>0.224589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282202</th>\n",
       "      <td>16874403</td>\n",
       "      <td>16874403</td>\n",
       "      <td>0.185245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282203</th>\n",
       "      <td>16874403</td>\n",
       "      <td>16874403</td>\n",
       "      <td>0.170537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282204</th>\n",
       "      <td>16874403</td>\n",
       "      <td>16874403</td>\n",
       "      <td>0.216598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84662 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ad_id  display_id  prediction\n",
       "197543  12392612    12392612    0.154566\n",
       "197544  12392612    12392612    0.146541\n",
       "197545  12392612    12392612    0.151116\n",
       "197546  12392612    12392612    0.178808\n",
       "197547  12392620    12392620    0.220336\n",
       "197548  12392620    12392620    0.203909\n",
       "197549  12392620    12392620    0.238116\n",
       "197550  12392620    12392620    0.226653\n",
       "197551  12392620    12392620    0.200199\n",
       "197552  12392620    12392620    0.176542\n",
       "197553  12393407    12393407    0.174452\n",
       "197554  12393407    12393407    0.167625\n",
       "197555  12393407    12393407    0.221674\n",
       "197556  12393407    12393407    0.133773\n",
       "197557  12393407    12393407    0.224631\n",
       "197558  12393407    12393407    0.132604\n",
       "197559  12393407    12393407    0.175955\n",
       "197560  12393407    12393407    0.144002\n",
       "197561  12393407    12393407    0.181854\n",
       "197562  12393434    12393434    0.175156\n",
       "197563  12393434    12393434    0.203909\n",
       "197564  12393434    12393434    0.238116\n",
       "197565  12393434    12393434    0.176959\n",
       "197566  12393434    12393434    0.176542\n",
       "197567  12393434    12393434    0.174757\n",
       "197568  12394160    12394160    0.159889\n",
       "197569  12394160    12394160    0.161158\n",
       "197570  12394160    12394160    0.161788\n",
       "197571  12394160    12394160    0.162451\n",
       "197572  12394160    12394160    0.161730\n",
       "...          ...         ...         ...\n",
       "282175  16872318    16872318    0.224827\n",
       "282176  16872318    16872318    0.229304\n",
       "282177  16872318    16872318    0.196075\n",
       "282178  16872318    16872318    0.170995\n",
       "282179  16872725    16872725    0.151234\n",
       "282180  16872725    16872725    0.153546\n",
       "282181  16872725    16872725    0.112192\n",
       "282182  16872725    16872725    0.150847\n",
       "282183  16873702    16873702    0.158565\n",
       "282184  16873702    16873702    0.151251\n",
       "282185  16873702    16873702    0.159101\n",
       "282186  16873702    16873702    0.180469\n",
       "282187  16873702    16873702    0.124899\n",
       "282188  16873702    16873702    0.161633\n",
       "282189  16873702    16873702    0.137283\n",
       "282190  16873702    16873702    0.160618\n",
       "282191  16873702    16873702    0.136593\n",
       "282192  16873702    16873702    0.159850\n",
       "282193  16873984    16873984    0.161903\n",
       "282194  16873984    16873984    0.133837\n",
       "282195  16873984    16873984    0.162191\n",
       "282196  16873984    16873984    0.161808\n",
       "282197  16873984    16873984    0.211302\n",
       "282198  16873984    16873984    0.211302\n",
       "282199  16874403    16874403    0.117902\n",
       "282200  16874403    16874403    0.212334\n",
       "282201  16874403    16874403    0.224589\n",
       "282202  16874403    16874403    0.185245\n",
       "282203  16874403    16874403    0.170537\n",
       "282204  16874403    16874403    0.216598\n",
       "\n",
       "[84662 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lr_prediction(test_data, test_labels, train_data, train_labels):\n",
    "    '''Returns the array of display_id, ad_id and probability it will be clicked'''\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_data[train_data.columns[11:]].fillna(0), train_labels)\n",
    "    lr_click_prob = lr.predict_proba(test_data[test_data.columns[11:]].fillna(0))[:,1]\n",
    "    lr_score = lr.score(test_data[test_data.columns[11:]].fillna(0), test_labels)\n",
    "    return pd.DataFrame({'display_id': test_data['display_id'],\n",
    "                         'ad_id': test_data['ad_id'],\n",
    "                         'prediction': lr_click_prob\n",
    "                        })\n",
    "lr_output = lr_prediction(test_data, test_labels, train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Format the data the way the submission requires\n",
    "output=test.groupby(['display_id'])['ad_id'].apply(lambda x:' '.join(map(str,x))).reset_index()\n",
    "\n",
    "# That's it for the simple solution (prior expectation)!\n",
    "output.to_csv('simplesolution3.cvs',index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
