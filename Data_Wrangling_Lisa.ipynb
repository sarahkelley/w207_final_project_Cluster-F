{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outbrain Click Prediction Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Submitted by Jay Cordes, Sarah Kelly, Nicole Lee, and Lisa Minas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Background and Modeling Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outbrain is looking to predict which ad in a given display will be clicked by a user. They have provided an extensive amount of information about where an ad is displayed, what the ad is displaying, and user activity on multiple publisher sites in the United States between 14-June-2016 and 28-June-2016. Based on this information, we are asked to order by decending probability which ad within a display a user will click. \n",
    "\n",
    "We plan to approach this problem in a step-wise process. First, we will explore the data to better understand what information has been provided. Based on our initial investigation, we will transform the data to fit with the machine learning models we expect to be most appropriate for predicting clicks. If we find potential value in multiple models, we will quickly score a trained model on development data to determine which model we should focus on for optimization. Once a final model is chosen, we will further refine the model and judiciously score against the development model. Our final score will be determined by testing against a provided test dataset (clicks_test) and by the score judged via Kaggle.\n",
    "\n",
    "Note: We are running this notebook in Python3 because Kaggle uses Python3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load all of the datasets and save them to files we will not overwrite. These files are very large and take a while to download. To avoid re-downloading multiple times through development, we found it easiest to create copies of the original files that we could re-refer to when needing to adjust our transformations.\n",
    "\n",
    "Based on documentation, platform and traffic_source should be 1 of three values: 1, 2, 3. We discovered through EDA that these numbers came in as both ints and strs. To clean them up, we added a dtype statement to the read_csv below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# importing general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# importing ML libraries\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# importing visual analysis \n",
    "from IPython.display import Image\n",
    "\n",
    "# # Other libraries from homework 2\n",
    "# SK-learn libraries for learning.\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# # SK-learn libraries for evaluation.\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # SK-learn libraries for feature extraction from text.\n",
    "# from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading all of the files\n",
    "clicks_train_og = pd.read_csv(\"../input/clicks_train.csv\")\n",
    "promoted_content_og = pd.read_csv(\"../input/promoted_content.csv\")\n",
    "doc_cats_og = pd.read_csv(\"../input/documents_categories.csv\")\n",
    "doc_ents_og = pd.read_csv(\"../input/documents_entities.csv\")\n",
    "doc_meta_og = pd.read_csv(\"../input/documents_meta.csv\")\n",
    "doc_topics_og = pd.read_csv(\"../input/documents_topics.csv\")\n",
    "events_og = pd.read_csv(\"../input/events.csv\", dtype={'platform': str, 'geo_location': str})\n",
    "page_views_og = pd.read_csv(\"../input/page_views_sample.csv\", dtype={'platform': str, 'traffic_source': str, 'geo_location': str})\n",
    "clicks_test_og = pd.read_csv(\"../input/clicks_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size has turned into a major issue with this project. Outbrain provided 9 CSVs of data averaging over 100 MB in compressed form with one file that is over 2 billion rows and 100GB uncompressed. This file is so large, Outbrain provided a 10th file which is a smaller sample to use for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Clicks_train size is: {}\".format(clicks_train_og.shape))\n",
    "print(\"Clicks_test size is: {}\".format(clicks_test_og.shape))\n",
    "print(\"Promoted Content size is: {}\".format(promoted_content_og.shape))\n",
    "print(\"Document Categories size is: {}\".format(doc_cats_og.shape))\n",
    "print(\"Document Entities size is: {}\".format(doc_ents_og.shape))\n",
    "print(\"Document Meta size is: {}\".format(doc_meta_og.shape))\n",
    "print(\"Document Topics size is: {}\".format(doc_topics_og.shape))\n",
    "print(\"Events size is: {}\".format(events_og.shape))\n",
    "print(\"Page Views size is: {}\".format(page_views_og.shape)) # note: full page views is ~100GB uncompressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore what we're training on: clicks_train and clicks test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Clicks_train:\")\n",
    "print(clicks_train_og.head())\n",
    "print(\"\\nClicks_test:\")\n",
    "print(clicks_test_og.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that test does not have the clicked label. To score our models as we build, we need to have a development set that we can predict and verify success against a known label. If we don't have a set-aside testing dataset, we'll likely overfit our model to the training data and have poorer results when scoring clicks_test and the hidden kaggle testind dataset. Therefore we will break the training data into training and development with a 70%-30% split. Otherwise, the two datasets look very similar. But display_id is the same for these first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clicks_train_og[clicks_train_og.display_id < 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few display_ids show us that there's multiple ad_ids per display_id. Within each display_id grouping, only one ad_id is which is marked as checked. Can a single ad_id be in multiple display_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clicks_train_og[clicks_train_og.ad_id == 250082]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, a single ad_id can appear in many different display_ids and each ad_id can be clicked on in multiple different display_ids. To learn more about any ad_id, Outbrain says to look into promoted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "promoted_content_og.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promoted content shows which campaign_id from which advertiser_id contains the ad_id. Each campaign_id can contain multiple ad_ids. Can an ad_id be related to multiple document_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Are there multiple rows per ad?: {}\".format(len(promoted_content_og['ad_id'].unique()) != len(promoted_content_og)))\n",
    "print(\"Is each document related to only one ad?: {}\".format(len(promoted_content_og['document_id'].unique()) == len(promoted_content_og)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a document? Outbrain says the document_X.csv files provide context on documents as well as Outbrain's confidence in each respective relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Document Categories:\")\n",
    "print(doc_cats_og.head())\n",
    "print(\"\\nDocument Entities:\")\n",
    "print(doc_ents_og.head())\n",
    "print(\"\\nDocument Topics:\")\n",
    "print(doc_topics_og.head())\n",
    "print(\"\\nDocument Meta:\")\n",
    "print(doc_meta_og.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each document_id can be described by a list of category_ids, entity_ids, and topic_ids. Apparently Outbrain either doesn't fully know what is in each document_id or they judge a document is mostly defined by one aspect but has influences of another. As an example: if a document talks about Brad Pitt and refers to his ex-wife as Angelina, we can say the document is 80% about Brad Pitt and 20% about Angelina. Or, alternatively, they are 80% sure the words _Brad Pitt_ refers to the person Brad Pitt and 20% sure the words _Angelina_ refers to the person Angelina Jole. Either interpretation will work for strength of relationship between a document aspect and the document.\n",
    "\n",
    "The meta file is in a different format than the other document files. This one shows specific details on the document. Based on the documentation on Kaggle, source_id is the full home address, publisher_id is the parent of source, and publish_time is when the document was launched on the source. So, for example, webpage with address _edition.cnn.com/20160623/some_news_article/index.html_ has a source_id=edition.cnn.com, a publisher_id=cnn.com, a publish_time=June 23 2016, and a document_id=some_news_article/index.html. These values are masked by numbers, but this example helps keep it straight for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've discussed what is refered to by an ad_id and a document_id, but where does the display_id and user come into this? Remember, our final output is to rank in decending order of click probability which ad_id will be clicked within the context of a specific **display_id**, not document_id. Outbrain says events covers display context. A display in clicks_train or clicks_test means one ad from the display was clicked. Therefore, events covers all events where at least one ad was clicked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "events_og.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('How many rows are there in events?: {:,}'.format(len(events_og)))\n",
    "print('How many users are there?: {:,}'.format(len(events_og.uuid.unique())))\n",
    "print('How many documents are there?: {:,}'.format(len(events_og.document_id.unique())))\n",
    "print('How many displays are there?: {:,}'.format(len(events_og.display_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seen = set()\n",
    "dup = set()\n",
    "for i in events_og.uuid:\n",
    "    if i in seen:\n",
    "        dup.add(i)\n",
    "    seen.add(i)\n",
    "events_og[events_og.uuid.isin(dup)].sort_values(by='uuid')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same user can be shown multiple different displays, but this happens rarely and display_id is unique per user_id * document_id. A display_id is associated with multiple documents which we interpret as a document can be pointed to by multiple displays. But since an ad does the actual pointing, an ad can appear in different displays and point to the same document. This is all pretty confusing so here's a diagram to help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(filename='relationship_diagram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything interesting about the users in page_views?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "page_views_og.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that clicks are not mentioned here. This indicates page_views is about _any_ page view, not just the ones with a 'success' of a click. Hence the full page_views file is 100GB - there were a lot of documents viewed that never received a click."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we have, it's time to join the data. The relationships are complicated due to potential multiple joins and document_id having two different definitions depending on which table it comes from. Therefore, we join tables in a step-wise fashion, checking that the key field per tale is included as we build up our working dataset. Please see inline comments for what happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Documents are highest level item in the dataset.\n",
    "# To find the full set of documents covered by our datasets, \n",
    "# pull in the documents that are in both page_views and promoted_content.\n",
    "doc_ids = set(page_views_og['document_id']) & set(promoted_content_og['document_id'])\n",
    "\n",
    "# To only view displays and ads that are on our 'master' document list,\n",
    "# filter events to documents that are found in page_views and promoted_content.\n",
    "events = events_og[events_og['document_id'].isin(doc_ids)]\n",
    "\n",
    "# We only want to view ad clicks for displays that have document information.\n",
    "# So we filter clicks to displays found in events.\n",
    "clicks_train = clicks_train_og[clicks_train_og['display_id'].isin(events['display_id'])]\n",
    "\n",
    "# Because clicks may not cover all displays shown in events,\n",
    "# re-filter events to display_id's that are found in clicks_train.\n",
    "events = events_og[events_og['display_id'].isin(clicks_train['display_id'])]\n",
    "\n",
    "# By this point we have a solid list of ads, displays, and documents found in our activity files.\n",
    "# Now filter promoted content to ads found in the filtered training dataset for model fitting.\n",
    "promoted_content = promoted_content_og[promoted_content_og['ad_id'].isin(clicks_train['ad_id'])]\n",
    "\n",
    "# We only need document information about documents we are still considering after prior filters.\n",
    "# Filter document content files to only documents that have made it through to promoted_content for space reasons.\n",
    "doc_cats = doc_cats_og[doc_cats_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_ents = doc_ents_og[doc_ents_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_meta = doc_meta_og[doc_meta_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_topics = doc_topics_og[doc_topics_og['document_id'].isin(promoted_content['document_id'])]\n",
    "\n",
    "# Similarly, filter page views to documents found in events to make this file manageable.\n",
    "page_views = page_views_og[page_views_og['document_id'].isin(events['document_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our EDA, we are planning to fit models INSERT WHAT WE ARE DOING HERE AND WHY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id   ad_id  clicked            uuid  document_id_x  timestamp  \\\n",
      "0          37   70153        0  d4f62cdcb39ad8        1779285       2687   \n",
      "1          37  149047        0  d4f62cdcb39ad8        1779285       2687   \n",
      "2          37  169564        0  d4f62cdcb39ad8        1779285       2687   \n",
      "3          37  234713        1  d4f62cdcb39ad8        1779285       2687   \n",
      "4          37  235443        0  d4f62cdcb39ad8        1779285       2687   \n",
      "\n",
      "  platform geo_location  document_id_y  campaign_id     ...      cat_2002  \\\n",
      "0        2    US>WA>819         933716         7516     ...           0.0   \n",
      "1        2    US>WA>819        1169985        16636     ...           0.0   \n",
      "2        2    US>WA>819        1394819        20109     ...           0.0   \n",
      "3        2    US>WA>819        1586431          245     ...           0.0   \n",
      "4        2    US>WA>819        1377696        11654     ...           0.0   \n",
      "\n",
      "   cat_2003  cat_2004  cat_2005  cat_2006  cat_2100  advertiser_perc  \\\n",
      "0       0.0       0.0       0.0       0.0       0.0         0.046069   \n",
      "1       0.0       0.0       0.0       0.0       0.0         0.137931   \n",
      "2       0.0       0.0       0.0       0.0       0.0         0.000000   \n",
      "3       0.0       0.0       0.0       0.0       0.0         0.271255   \n",
      "4       0.0       0.0       0.0       0.0       0.0         0.082631   \n",
      "\n",
      "   campaign_perc  docx_view_freq  click_perc  \n",
      "0       0.045455            7701    0.052632  \n",
      "1       0.137931            7701    0.076923  \n",
      "2       0.000000            7701    0.071429  \n",
      "3       0.265217            7701    0.281250  \n",
      "4       0.043478            7701    0.043478  \n",
      "\n",
      "[5 rows x 411 columns]\n"
     ]
    }
   ],
   "source": [
    "# first, format the training dataset\n",
    "\n",
    "def click_percent(dataset, ad_id, default_result, reg):\n",
    "    '''Returns the posterior probability of ad being clicked.\n",
    "    If ad has not been encountered before, assume mean click'''\n",
    "    \n",
    "    # count number of times ad has been seen\n",
    "    ad_total = len(dataset[dataset['ad_id'] == ad_id])\n",
    "    \n",
    "    # if ad has not been seen, returned the default_results\n",
    "    if ad_total == 0:\n",
    "        return default_result\n",
    "    # otherwise return percentage of times ad has been clicked, adjusted by a regularization term\n",
    "    else:\n",
    "        click_sum = np.sum(dataset[dataset['ad_id'] == ad_id].clicked) + 1.0\n",
    "        return click_sum / (ad_total + reg)\n",
    "\n",
    "# Merging information aout the displays to master dataset\n",
    "data = clicks_train.merge(events, on='display_id', how='left')\n",
    "# joins information about the display that the user saw\n",
    "# each display has a unique user id, doc id, and timestamp\n",
    "# events has the information about the display (who the user is, which site (document_id) it was on, when it was seen, from where, etc.)\n",
    "\n",
    "# Identifying which documents the ads refer to (aka destination documents)\n",
    "data = data.merge(promoted_content, on='ad_id', how='left')\n",
    "\n",
    "# Gather/bin data about the documents the ads refer to\n",
    "sparsetop = doc_topics.pivot(index='document_id', columns='topic_id', values='confidence_level')\n",
    "sparsetop.columns = ['top_' + str(col) for col in sparsetop.columns]\n",
    "\n",
    "sparsecat = doc_cats.pivot(index='document_id', columns='category_id', values='confidence_level')\n",
    "sparsecat.columns = ['cat_' + str(col) for col in sparsecat.columns]\n",
    "\n",
    "sparse = sparsetop.join(sparsecat, how='outer')\n",
    "sparse.fillna(0, inplace=True)\n",
    "del sparsetop, sparsecat\n",
    "\n",
    "sparse.reset_index(level=0, inplace=True)\n",
    "\n",
    "data = data.merge(sparse, left_on='document_id_y', right_on='document_id', how='left')\n",
    "\n",
    "# adding in advertiser information\n",
    "advr_success = dict(zip(data.advertiser_id.unique(),\n",
    "                        [sum(data[data['advertiser_id']==x]['clicked'])/len(data[data['advertiser_id']==x]) for x in data['advertiser_id'].unique()]))\n",
    "data['advertiser_perc'] = data['advertiser_id'].map(advr_success)\n",
    "\n",
    "# adding in campagin information\n",
    "camp_success = dict(zip(data.campaign_id.unique(), \n",
    "                        [sum(data[data['campaign_id']==x]['clicked'])/len(data[data['campaign_id']==x]) for x in data['campaign_id'].unique()]))\n",
    "data['campaign_perc'] = data['campaign_id'].map(camp_success)\n",
    "\n",
    "# adding in meta about document view frequencies\n",
    "doc_view_freq = dict(zip(page_views.document_id.unique(), \n",
    "                         [len(page_views[page_views.document_id==x]) for x in page_views.document_id.unique()]))\n",
    "data['docx_view_freq'] = data['document_id_x'].map(doc_view_freq)\n",
    "\n",
    "# Adding meta data about prior click percentage\n",
    "mean_click = np.mean(data[\"clicked\"])\n",
    "click_success = dict(zip(data.ad_id.unique(), \n",
    "                         [click_percent(data, x, mean_click, 10.0) for x in data[\"ad_id\"].unique()] ))\n",
    "data['click_perc'] = data['ad_id'].map(click_success)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   display_id   ad_id            uuid  document_id_x  timestamp platform  \\\n",
      "0    16874594   66758  16e1290e312561        1792425       1026        3   \n",
      "1    16874594  150083  16e1290e312561        1792425       1026        3   \n",
      "2    16874594  162754  16e1290e312561        1792425       1026        3   \n",
      "3    16874594  170392  16e1290e312561        1792425       1026        3   \n",
      "4    16874594  172888  16e1290e312561        1792425       1026        3   \n",
      "\n",
      "  geo_location  document_id_y  campaign_id  advertiser_id  \n",
      "0    US>MI>505        1051283         8949            555  \n",
      "1    US>MI>505        1358132        19045           1913  \n",
      "2    US>MI>505        1292723        17770           2391  \n",
      "3    US>MI>505        1083829        20943           1731  \n",
      "4    US>MI>505        1433954         1384             16  \n"
     ]
    }
   ],
   "source": [
    "# Pull from full events_og to make sure all display information is gathered\n",
    "data_test = clicks_test_og.merge(events_og, on='display_id', how='left')\n",
    "\n",
    "# find the ads from the entire promoted_content_og data and not the one filtered on clicks_train\n",
    "data_test = data_test.merge(promoted_content_og, on='ad_id', how='left')\n",
    "\n",
    "# THE BELOW CODE BREAKS WHEN RUNNING WITH THE FULL CLICKS_TEST\n",
    "\n",
    "# # matching top_X and cat_X categories from training to test. Sparse was created while developing the training dataset\n",
    "# data_test = data_test.merge(sparse, left_on='document_id_y', right_on='document_id', how='left')\n",
    "# print(data_test.display_id.unique().size)\n",
    "\n",
    "# # Adding meta data from training\n",
    "# data_test['docx_view_freq'] = data_test['document_id'].map(doc_view_freq)\n",
    "# data_test['campaign_perc'] = data_test['campaign_id'].map(camp_success)\n",
    "# data_test['advertiser_perc'] = data_test['advertiser_id'].map(advr_success)\n",
    "# data_test['click_perc'] = data_test['ad_id'].map(click_success)\n",
    "\n",
    "# # fill any nas so the modeling won't get wonky\n",
    "# data_test.fillna(0, inplace=True)\n",
    "# # check if there are any missing columns between the two datasets\n",
    "\n",
    "# missing_col = set(data_test.columns) ^ set(data.columns)\n",
    "# print(missing_col)\n",
    "\n",
    "# # only one missing, clicked. But that's ok because we're going to take clicked out to make a training and development set for modeling\n",
    "print(data_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training into train and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels length: 282205\n",
      "data length: (282205, 411)\n",
      "\n",
      "training label shape: (197543, 1)\n",
      "training data shape: (197543, 410)\n",
      "test label shape: (84662, 1)\n",
      "test data shape: (84662, 410)\n"
     ]
    }
   ],
   "source": [
    "def split_train_dev(dataset, train_percent = 0.7):\n",
    "    '''Splitting data into test and dev.\n",
    "    If train_precent is left at default, 70% will go into training and 30% into development.\n",
    "    If train_percent = 1, all data will go into training.\n",
    "    If train_percent = 0, all data will go into development.'''\n",
    "\n",
    "    labels = data['clicked']\n",
    "    labels = labels.values.reshape(-1,1)\n",
    "\n",
    "    print ('Labels length:', len(labels))\n",
    "    print ('data length:', data.shape)\n",
    "    print ('')\n",
    "        \n",
    "    train_data = data[:int(train_percent*len(data))].drop('clicked', 1)\n",
    "    dev_data = data[int(train_percent*len(data)):].drop('clicked', 1)\n",
    "\n",
    "    train_labels = labels[:int(train_percent*len(data))]\n",
    "    dev_labels = labels[int(train_percent*len(data)):]\n",
    "\n",
    "    print ('training label shape:', train_labels.shape)\n",
    "    print ('training data shape:', train_data.shape)\n",
    "    print ('test label shape:', dev_labels.shape)\n",
    "    print ('test data shape:', dev_data.shape)\n",
    "    \n",
    "    return train_data, train_labels, dev_data, dev_labels\n",
    "\n",
    "train_data, train_labels, dev_data, dev_labels = split_train_dev(data, train_percent = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have three datasets\n",
    "1. train_data, train_labels = 70% of clicks_train dataset. Used for training models.\n",
    "2. dev_data, dev_labels = remaining 30% of clicks_train dataset. Used for measuring accuracy of models on non-train data.\n",
    "3. test_data = clicks_test dataset for final scoring of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to print any of these datasets to csv, uncomment the appropriate line\n",
    "\n",
    "# data_test.to_csv('testingdata.cvs',index=False)\n",
    "\n",
    "# data.to_csv('fulltrainingdata.cvs',index=False)\n",
    "\n",
    "# train_data.to_csv('trainingdata.cvs',index=False)\n",
    "# train_labels.to_csv('traininglabels.cvs',index=False)\n",
    "\n",
    "# dev_data.to_csv('developmentdata.cvs',index=False)\n",
    "# dev_labels.to_csv('developmentlabels.cvs',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Discuss why Logistic regression seems like a good idea and what basic approach will be.** TO BE ADDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del doc_cats, doc_ents, doc_meta, doc_topics, promoted_content, events, page_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lisam\\Anaconda2\\envs\\py35\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting with 0 100000\n",
      "output (514114, 3)\n",
      "finished with (514114, 3)\n",
      "starting with 100000 200000\n",
      "output (524890, 3)\n",
      "finished with (1039004, 3)\n",
      "starting with 200000 300000\n",
      "output (511203, 3)\n",
      "finished with (1550207, 3)\n",
      "starting with 300000 400000\n",
      "output (517584, 3)\n",
      "finished with (2067791, 3)\n",
      "starting with 400000 500000\n",
      "output (514534, 3)\n",
      "finished with (2582325, 3)\n",
      "starting with 500000 600000\n",
      "output (513592, 3)\n",
      "finished with (3095917, 3)\n",
      "starting with 600000 700000\n",
      "output (521877, 3)\n",
      "finished with (3617794, 3)\n",
      "starting with 700000 800000\n",
      "output (511417, 3)\n",
      "finished with (4129211, 3)\n",
      "starting with 800000 900000\n",
      "output (521235, 3)\n",
      "finished with (4650446, 3)\n",
      "starting with 900000 1000000\n",
      "output (507411, 3)\n",
      "finished with (5157857, 3)\n",
      "starting with 1000000 1100000\n",
      "output (511411, 3)\n",
      "finished with (5669268, 3)\n",
      "starting with 1100000 1200000\n",
      "output (510785, 3)\n",
      "finished with (6180053, 3)\n",
      "starting with 1200000 1300000\n",
      "output (508638, 3)\n",
      "finished with (6688691, 3)\n",
      "starting with 1300000 1400000\n",
      "output (507408, 3)\n",
      "finished with (7196099, 3)\n",
      "starting with 1400000 1500000\n",
      "output (524457, 3)\n",
      "finished with (7720556, 3)\n",
      "starting with 1500000 1600000\n",
      "output (517598, 3)\n",
      "finished with (8238154, 3)\n",
      "starting with 1600000 1700000\n",
      "output (513866, 3)\n",
      "finished with (8752020, 3)\n",
      "starting with 1700000 1800000\n",
      "output (526161, 3)\n",
      "finished with (9278181, 3)\n",
      "starting with 1800000 1900000\n",
      "output (512040, 3)\n",
      "finished with (9790221, 3)\n",
      "starting with 1900000 2000000\n",
      "output (527051, 3)\n",
      "finished with (10317272, 3)\n",
      "starting with 2000000 2100000\n",
      "output (516871, 3)\n",
      "finished with (10834143, 3)\n",
      "starting with 2100000 2200000\n",
      "output (517562, 3)\n",
      "finished with (11351705, 3)\n",
      "starting with 2200000 2300000\n",
      "output (521540, 3)\n",
      "finished with (11873245, 3)\n",
      "starting with 2300000 2400000\n",
      "output (509904, 3)\n",
      "finished with (12383149, 3)\n",
      "starting with 2400000 2500000\n",
      "output (520823, 3)\n",
      "finished with (12903972, 3)\n",
      "starting with 2500000 2600000\n",
      "output (521738, 3)\n",
      "finished with (13425710, 3)\n",
      "starting with 2600000 2700000\n",
      "output (519437, 3)\n",
      "finished with (13945147, 3)\n",
      "starting with 2700000 2800000\n",
      "output (515907, 3)\n",
      "finished with (14461054, 3)\n",
      "starting with 2800000 2900000\n",
      "output (510187, 3)\n",
      "finished with (14971241, 3)\n",
      "starting with 2900000 3000000\n",
      "output (514326, 3)\n",
      "finished with (15485567, 3)\n",
      "starting with 3000000 3100000\n",
      "output (512500, 3)\n",
      "finished with (15998067, 3)\n",
      "starting with 3100000 3200000\n",
      "output (510541, 3)\n",
      "finished with (16508608, 3)\n",
      "starting with 3200000 3300000\n",
      "output (503263, 3)\n",
      "finished with (17011871, 3)\n",
      "starting with 3300000 3400000\n",
      "output (507287, 3)\n",
      "finished with (17519158, 3)\n",
      "starting with 3400000 3500000\n",
      "output (513818, 3)\n",
      "finished with (18032976, 3)\n",
      "starting with 3500000 3600000\n",
      "output (514882, 3)\n",
      "finished with (18547858, 3)\n",
      "starting with 3600000 3700000\n",
      "output (520534, 3)\n",
      "finished with (19068392, 3)\n",
      "starting with 3700000 3800000\n",
      "output (521911, 3)\n",
      "finished with (19590303, 3)\n",
      "starting with 3800000 3900000\n",
      "output (523614, 3)\n",
      "finished with (20113917, 3)\n",
      "starting with 3900000 4000000\n",
      "output (528358, 3)\n",
      "finished with (20642275, 3)\n",
      "starting with 4000000 4100000\n",
      "output (529383, 3)\n",
      "finished with (21171658, 3)\n",
      "starting with 4100000 4200000\n",
      "output (519730, 3)\n",
      "finished with (21691388, 3)\n",
      "starting with 4200000 4300000\n",
      "output (516599, 3)\n",
      "finished with (22207987, 3)\n",
      "starting with 4300000 4400000\n",
      "output (514236, 3)\n",
      "finished with (22722223, 3)\n",
      "starting with 4400000 4500000\n",
      "output (511557, 3)\n",
      "finished with (23233780, 3)\n",
      "starting with 4500000 4600000\n",
      "output (510392, 3)\n",
      "finished with (23744172, 3)\n",
      "starting with 4600000 4700000\n",
      "output (510449, 3)\n",
      "finished with (24254621, 3)\n",
      "starting with 4700000 4800000\n",
      "output (517358, 3)\n",
      "finished with (24771979, 3)\n",
      "starting with 4800000 4900000\n",
      "output (511067, 3)\n",
      "finished with (25283046, 3)\n",
      "starting with 4900000 5000000\n",
      "output (508543, 3)\n",
      "finished with (25791589, 3)\n",
      "starting with 5000000 5100000\n",
      "output (512437, 3)\n",
      "finished with (26304026, 3)\n",
      "starting with 5100000 5200000\n",
      "output (513232, 3)\n",
      "finished with (26817258, 3)\n",
      "starting with 5200000 5300000\n",
      "output (517397, 3)\n",
      "finished with (27334655, 3)\n",
      "starting with 5300000 5400000\n",
      "output (516731, 3)\n",
      "finished with (27851386, 3)\n",
      "starting with 5400000 5500000\n",
      "output (515972, 3)\n",
      "finished with (28367358, 3)\n",
      "starting with 5500000 5600000\n",
      "output (521114, 3)\n",
      "finished with (28888472, 3)\n",
      "starting with 5600000 5700000\n",
      "output (520267, 3)\n",
      "finished with (29408739, 3)\n",
      "starting with 5700000 5800000\n",
      "output (523849, 3)\n",
      "finished with (29932588, 3)\n",
      "starting with 5800000 5900000\n",
      "output (522060, 3)\n",
      "finished with (30454648, 3)\n",
      "starting with 5900000 6000000\n",
      "output (514052, 3)\n",
      "finished with (30968700, 3)\n",
      "starting with 6000000 6100000\n",
      "output (512532, 3)\n",
      "finished with (31481232, 3)\n",
      "starting with 6100000 6200000\n",
      "output (512442, 3)\n",
      "finished with (31993674, 3)\n",
      "starting with 6200000 6245533\n",
      "output (231488, 3)\n",
      "finished with (32225162, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32225162, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clicks_test is too large to run (over 6M rows breaks my poor computer's memory) so we're doing it in batches\n",
    "\n",
    "def lr_output(dataset, model):\n",
    "    \n",
    "    # matching top_X and cat_X categories from training to test. Sparse was created while developing the training dataset.\n",
    "    dataset = dataset.merge(sparse, left_on='document_id_y', right_on='document_id', how='left')\n",
    "    \n",
    "    # Adding meta data from training\n",
    "    dataset['docx_view_freq'] = dataset['document_id_x'].map(doc_view_freq)\n",
    "    dataset['campaign_perc'] = dataset['campaign_id'].map(camp_success)\n",
    "    dataset['advertiser_perc'] = dataset['advertiser_id'].map(advr_success)\n",
    "    dataset['click_perc'] = dataset['ad_id'].map(click_success)\n",
    "    \n",
    "    # fill any nas so the modeling won't get wonky\n",
    "    dataset.fillna(0, inplace=True)\n",
    "    \n",
    "    # check if there are any missing columns between the two datasets\n",
    "    if set(dataset.columns) ^ set(train_data.columns) :\n",
    "        print('We have a problem here!')\n",
    "        \n",
    "    click_prob = pd.DataFrame({'display_id': dataset['display_id'],\n",
    "                               'ad_id': dataset['ad_id'],\n",
    "                               'prediction': lr.predict_proba(dataset[dataset.columns[11:]].fillna(0))[:,1]\n",
    "                              })\n",
    "    \n",
    "    return click_prob\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_data[train_data.columns[11:]].fillna(0), train_labels)\n",
    "\n",
    "lr_click_probs = pd.DataFrame({'display_id': [], 'ad_id': [], 'prediction': []})\n",
    "display_ids = data_test.display_id.unique()\n",
    "min_id, max_id, display_max = 0, 100000, len(display_ids)\n",
    "\n",
    "while min_id != display_max:\n",
    "\n",
    "    # keep index within length of display_ids\n",
    "    max_id = min(max_id,display_max)\n",
    "    \n",
    "    # grab that subset of idsplay_ids\n",
    "    display_subset = display_ids[min_id:max_id]\n",
    "    \n",
    "    # find the subset\n",
    "    print('starting with', min_id, max_id)\n",
    "    output = lr_output(data_test[data_test.display_id.isin(display_subset)], lr)\n",
    "    print('output', output.shape)\n",
    "    lr_click_probs = lr_click_probs.append(output, ignore_index = True)\n",
    "    \n",
    "    print('finished with', lr_click_probs.shape)\n",
    "    min_id = max_id\n",
    "    max_id += 100000\n",
    "\n",
    "print(lr_click_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort predictions\n",
    "lr_click_probs.sort_values(by=['display_id','prediction'], ascending=False, inplace=True)\n",
    "\n",
    "# Format the data the way the submission requires\n",
    "output=lr_click_probs.groupby(['display_id'])['ad_id'].apply(lambda x:' '.join(map(str,x))).reset_index()\n",
    "\n",
    "# That's it for the simple solution (prior expectation)!\n",
    "output.to_csv('submission_20161211_1644.cvs',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix/Working space - this is stuff that hasn't been added to the 'full' run.\n",
    "### AKA Lisa's stuff that might go somewhere or might be dumped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lost comments\n",
    "** The below are all good comments that have become detached from their original location, probably from Lisa creating the dataload.py. Keeping them here for now so that they can be reattached as we work on formating the final notebook.**\n",
    "\n",
    "---\n",
    "\n",
    "Now we are merging information on what documents the ads referred to (from source: promoted_content).  \n",
    "In every display, there are multiple ads (within one document = document_id_x). Every ad refers to a different document, which is the site the ad is promoting (document_id_y). All the columns after document_id_y are information about that document (to which the ad is referring).\n",
    "\n",
    "---\n",
    "\n",
    "### Merge information about the documents the ads refer to\n",
    "All the doc files have information about the documents (websites) to which the ads refer to\n",
    "including confidence levels of which topics the ads referred to, which categories they're apart of, etc.\n",
    "\n",
    "We wanted to duplicate the idea of the CountVectorizer for the 'bag of words' model we used for spam detection, but since we're not counting words in a text, it's a little bit different. Since we have a 'dictionary' of categories and topics, we use that as our 'vocabulary.' Every document has a confidence level for one or more items in the vocabulary, so we create a sparse matrix with every topic and category as columns, and every document has a confidence level value in the respective columns. If they are not given a confidence level, we put 0 because the document most likely does not have anything to do with that category or topic (given the data provided by Outbrain).\n",
    "\n",
    "This data on the documents will help us separate ads from one another based on topic/category.  \n",
    "ie) why did ad A get clicked instead of ad B? We know ad A referred to document 1 whereas ad B referred to document 2, and now we have general information about the documents the ads referred to. We will merge this information in later steps.\n",
    "\n",
    "---\n",
    "\n",
    "creating dictionaries for % of ads clicked for every advertiser and campaign.  \n",
    "purpose: merge to master dataset as a feature for every ad, how often the advertiser and campaign are successful on average.\n",
    "\n",
    "---\n",
    "\n",
    "Add count of page views to every document that an ad appears in (document_id_x) as a feature, could tell us something about likelihood of ads being clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #################3 IGNORE THIS#############3\n",
    "\n",
    "# # events['country'], events['state'] = zip(*map(lambda x: str(x).split('>'), list(events['geo_location'])))\n",
    "\n",
    "# # #temp = map(lambda x: str(x).split('>'), list(page_views['geo_location']))\n",
    "# # print temp[:5]\n",
    "# # zip(*temp[:5]) # removes DMA\n",
    "\n",
    "# #events2 = pd.DataFrame(events['geo_location'].str.split(',').tolist(), columns = ['country', 'state', 'dma'])\n",
    "# events['geo_location'].str.split('>', expand= True)\n",
    "# geo = map(lambda x: str(x).split('>'), events['geo_location'])\n",
    "# zip(*geo)\n",
    "# country = [x[0] for x in geo]\n",
    "# #state = [x[1] for x in geo if x[1]]\n",
    "# if None:\n",
    "#     print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def lr_prediction(train_data, train_labels, test_data, test_labels):\n",
    "    '''Returns the array of display_id, ad_id and probability it will be clicked'''\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_data[train_data.columns[11:]].fillna(0), train_labels)\n",
    "    lr_click_prob = lr.predict_proba(test_data[test_data.columns[11:]].fillna(0))[:,1]\n",
    "    lr_score = lr.score(test_data[test_data.columns[11:]].fillna(0), test_labels)\n",
    "    return pd.DataFrame({'display_id': test_data['display_id'],\n",
    "                         'ad_id': test_data['ad_id'],\n",
    "                         'prediction': lr_click_prob\n",
    "                        })\n",
    "lr_output = lr_prediction(train_data, train_labels, dev_data, dev_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
