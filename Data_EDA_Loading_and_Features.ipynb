{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outbrain Click Prediction Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Submitted by Jay Cordes, Sarah Kelly, Nicole Lee, and Lisa Minas_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Background and Modeling Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outbrain is looking to predict which ad in a given display will be clicked by a user. They have provided an extensive amount of information about where an ad is displayed, what the ad is displaying, and user activity on multiple publisher sites in the United States between 14-June-2016 and 28-June-2016. Based on this information, we are asked to order by decending probability which ad within a display a user will click. \n",
    "\n",
    "We plan to approach this problem in a step-wise process. First, we will explore the data to better understand what information has been provided. Based on our initial investigation, we will transform the data to fit with the machine learning models we expect to be most appropriate for predicting clicks. If we find potential value in multiple models, we will quickly score a trained model on development data to determine which model we should focus on for optimization. Once a final model is chosen, we will further refine the model and judiciously score against the development model. Our final score will be determined by testing against a provided test dataset (clicks_test) and by the score judged via Kaggle.\n",
    "\n",
    "Note: We are running this notebook in Python3 because Kaggle uses Python3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load all of the datasets and save them to files we will not overwrite. These files are very large and take a while to download. To avoid re-downloading multiple times through development, we found it easiest to create copies of the original files that we could re-refer to when needing to adjust our transformations.\n",
    "\n",
    "Based on documentation, platform and traffic_source should be 1 of three values: 1, 2, 3. We discovered through EDA that these numbers came in as both ints and strs. To clean them up, we added a dtype statement to the read_csv below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# importing general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# importing ML libraries\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# importing visual analysis \n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading all of the files\n",
    "clicks_train_og = pd.read_csv(\"../input/clicks_train.csv\")\n",
    "promoted_content_og = pd.read_csv(\"../input/promoted_content.csv\")\n",
    "doc_cats_og = pd.read_csv(\"../input/documents_categories.csv\")\n",
    "doc_ents_og = pd.read_csv(\"../input/documents_entities.csv\")\n",
    "doc_meta_og = pd.read_csv(\"../input/documents_meta.csv\")\n",
    "doc_topics_og = pd.read_csv(\"../input/documents_topics.csv\")\n",
    "events_og = pd.read_csv(\"../input/events.csv\", dtype={'platform': str, 'geo_location': str})\n",
    "page_views_og = pd.read_csv(\"../input/page_views_sample.csv\", dtype={'platform': str, 'traffic_source': str, 'geo_location': str})\n",
    "clicks_test_og = pd.read_csv(\"../input/clicks_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size has turned into a major issue with this project. Outbrain provided 9 CSVs of data averaging over 100 MB in compressed form with one file that is over 2 billion rows and 100GB uncompressed. This file is so large, Outbrain provided a 10th file which is a smaller sample to use for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicks_train size is: (87141731, 3)\n",
      "Clicks_test size is: (32225162, 2)\n",
      "Promoted Content size is: (559583, 4)\n",
      "Document Categories size is: (5481475, 3)\n",
      "Document Entities size is: (5537552, 3)\n",
      "Document Meta size is: (2999334, 4)\n",
      "Document Topics size is: (11325960, 3)\n",
      "Events size is: (23120126, 6)\n",
      "Page Views size is: (9999999, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Clicks_train size is: {}\".format(clicks_train_og.shape))\n",
    "print(\"Clicks_test size is: {}\".format(clicks_test_og.shape))\n",
    "print(\"Promoted Content size is: {}\".format(promoted_content_og.shape))\n",
    "print(\"Document Categories size is: {}\".format(doc_cats_og.shape))\n",
    "print(\"Document Entities size is: {}\".format(doc_ents_og.shape))\n",
    "print(\"Document Meta size is: {}\".format(doc_meta_og.shape))\n",
    "print(\"Document Topics size is: {}\".format(doc_topics_og.shape))\n",
    "print(\"Events size is: {}\".format(events_og.shape))\n",
    "print(\"Page Views size is: {}\".format(page_views_og.shape)) # note: full page views is ~100GB uncompressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore what we're training on: clicks_train and clicks test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicks_train:\n",
      "   display_id   ad_id  clicked\n",
      "0           1   42337        0\n",
      "1           1  139684        0\n",
      "2           1  144739        1\n",
      "3           1  156824        0\n",
      "4           1  279295        0\n",
      "\n",
      "Clicks_test:\n",
      "   display_id   ad_id\n",
      "0    16874594   66758\n",
      "1    16874594  150083\n",
      "2    16874594  162754\n",
      "3    16874594  170392\n",
      "4    16874594  172888\n"
     ]
    }
   ],
   "source": [
    "print(\"Clicks_train:\")\n",
    "print(clicks_train_og.head())\n",
    "print(\"\\nClicks_test:\")\n",
    "print(clicks_test_og.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that test does not have the clicked label. To score our models as we build, we need to have a development set that we can predict and verify success against a known label. If we don't have a set-aside testing dataset, we'll likely overfit our model to the training data and have poorer results when scoring clicks_test and the hidden kaggle testind dataset. Therefore we will break the training data into training and development with a 70%-30% split. Otherwise, the two datasets look very similar. But display_id is the same for these first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    display_id   ad_id  clicked\n",
      "0            1   42337        0\n",
      "1            1  139684        0\n",
      "2            1  144739        1\n",
      "3            1  156824        0\n",
      "4            1  279295        0\n",
      "5            1  296965        0\n",
      "6            2  125211        0\n",
      "7            2  156535        0\n",
      "8            2  169564        0\n",
      "9            2  308455        1\n",
      "10           3   71547        0\n",
      "11           3   95814        0\n",
      "12           3  152141        0\n",
      "13           3  183846        0\n",
      "14           3  228657        1\n",
      "15           3  250082        0\n"
     ]
    }
   ],
   "source": [
    "print(clicks_train_og[clicks_train_og.display_id < 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few display_ids show us that there's multiple ad_ids per display_id. Within each display_id grouping, only one ad_id is which is marked as checked. Can a single ad_id be in multiple display_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       display_id   ad_id  clicked\n",
      "15              3  250082        0\n",
      "441            91  250082        0\n",
      "550           112  250082        0\n",
      "6297         1241  250082        0\n",
      "7732         1526  250082        0\n",
      "9539         1874  250082        0\n",
      "11730        2299  250082        0\n",
      "12850        2516  250082        0\n",
      "14723        2878  250082        0\n",
      "15398        3008  250082        0\n",
      "17683        3446  250082        1\n",
      "18346        3579  250082        0\n",
      "19724        3839  250082        0\n",
      "20174        3926  250082        0\n",
      "21556        4196  250082        0\n",
      "22733        4435  250082        0\n",
      "28623        5566  250082        0\n",
      "31803        6194  250082        0\n",
      "32951        6427  250082        0\n",
      "33462        6529  250082        0\n"
     ]
    }
   ],
   "source": [
    "print(clicks_train_og[clicks_train_og.ad_id == 250082].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, a single ad_id can appear in many different display_ids and each ad_id can be clicked on in multiple different display_ids. To learn more about any ad_id, Outbrain says to look into promoted content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ad_id  document_id  campaign_id  advertiser_id\n",
      "0      1         6614            1              7\n",
      "1      2       471467            2              7\n",
      "2      3         7692            3              7\n",
      "3      4       471471            2              7\n",
      "4      5       471472            2              7\n"
     ]
    }
   ],
   "source": [
    "print(promoted_content_og.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promoted content shows which campaign_id from which advertiser_id contains the ad_id. Each campaign_id can contain multiple ad_ids. Can an ad_id be related to multiple document_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there multiple rows per ad?: False\n",
      "Is each document related to only one ad?: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Are there multiple rows per ad?: {}\".format(len(promoted_content_og['ad_id'].unique()) != len(promoted_content_og)))\n",
    "print(\"Is each document related to only one ad?: {}\".format(len(promoted_content_og['document_id'].unique()) == len(promoted_content_og)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a document? Outbrain says the document_X.csv files provide context on documents as well as Outbrain's confidence in each respective relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Categories:\n",
      "   document_id  category_id  confidence_level\n",
      "0      1595802         1611              0.92\n",
      "1      1595802         1610              0.07\n",
      "2      1524246         1807              0.92\n",
      "3      1524246         1608              0.07\n",
      "4      1617787         1807              0.92\n",
      "\n",
      "Document Entities:\n",
      "   document_id                         entity_id  confidence_level\n",
      "0      1524246  f9eec25663db4cd83183f5c805186f16          0.672865\n",
      "1      1524246  55ebcfbdaff1d6f60b3907151f38527a          0.399114\n",
      "2      1524246  839907a972930b17b125eb0247898412          0.392096\n",
      "3      1524246  04d8f9a1ad48f126d5806a9236872604          0.213996\n",
      "4      1617787  612a1d17685a498aff4f036c1ee02c16          0.386193\n",
      "\n",
      "Document Topics:\n",
      "   document_id  topic_id  confidence_level\n",
      "0      1595802       140          0.073113\n",
      "1      1595802        16          0.059416\n",
      "2      1595802       143          0.045421\n",
      "3      1595802       170          0.038867\n",
      "4      1524246       113          0.196450\n",
      "\n",
      "Document Meta:\n",
      "   document_id  source_id  publisher_id         publish_time\n",
      "0      1595802          1           603  2016-06-05 00:00:00\n",
      "1      1524246          1           603  2016-05-26 11:00:00\n",
      "2      1617787          1           603  2016-05-27 00:00:00\n",
      "3      1615583          1           603  2016-06-07 00:00:00\n",
      "4      1615460          1           603  2016-06-20 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Document Categories:\")\n",
    "print(doc_cats_og.head())\n",
    "print(\"\\nDocument Entities:\")\n",
    "print(doc_ents_og.head())\n",
    "print(\"\\nDocument Topics:\")\n",
    "print(doc_topics_og.head())\n",
    "print(\"\\nDocument Meta:\")\n",
    "print(doc_meta_og.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each document_id can be described by a list of category_ids, entity_ids, and topic_ids. Apparently Outbrain either doesn't fully know what is in each document_id or they judge a document is mostly defined by one aspect but has influences of another. As an example: if a document talks about Brad Pitt and refers to his ex-wife as Angelina, we can say the document is 80% about Brad Pitt and 20% about Angelina. Or, alternatively, they are 80% sure the words _Brad Pitt_ refers to the person Brad Pitt and 20% sure the words _Angelina_ refers to the person Angelina Jole. Either interpretation will work for strength of relationship between a document aspect and the document.\n",
    "\n",
    "The meta file is in a different format than the other document files. This one shows specific details on the document. Based on the documentation on Kaggle, source_id is the full home address, publisher_id is the parent of source, and publish_time is when the document was launched on the source. So, for example, webpage with address _edition.cnn.com/20160623/some_news_article/index.html_ has a source_id=edition.cnn.com, a publisher_id=cnn.com, a publish_time=June 23 2016, and a document_id=some_news_article/index.html. These values are masked by numbers, but this example helps keep it straight for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've discussed what is refered to by an ad_id and a document_id, but where does the display_id and user come into this? Remember, our final output is to rank in decending order of click probability which ad_id will be clicked within the context of a specific **display_id**, not document_id. Outbrain says events covers display context. A display in clicks_train or clicks_test means one ad from the display was clicked. Therefore, events covers all events where at least one ad was clicked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          display_id            uuid  document_id   timestamp platform  \\\n",
      "23120121    23120122  3b42aaa4aa8993      1827718  1295999574        1   \n",
      "23120122    23120123  7efccdc2d58fd1      2984543  1295999591        2   \n",
      "23120123    23120124  11f9ac8cee26f2       751048  1295999657        2   \n",
      "23120124    23120125  6bbdc5756789cc       744496  1295999771        2   \n",
      "23120125    23120126  b545c100626cba      2357447  1295999805        2   \n",
      "\n",
      "         geo_location  \n",
      "23120121    US>FL>571  \n",
      "23120122    US>TX>623  \n",
      "23120123    US>GA>524  \n",
      "23120124    US>MI>505  \n",
      "23120125    US>SD>764  \n"
     ]
    }
   ],
   "source": [
    "print(events_og.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many rows are there in events?: 23,120,126\n",
      "How many users are there?: 19,794,967\n",
      "How many documents are there?: 894,060\n",
      "How many displays are there?: 23,120,126\n"
     ]
    }
   ],
   "source": [
    "print('How many rows are there in events?: {:,}'.format(len(events_og)))\n",
    "print('How many users are there?: {:,}'.format(len(events_og.uuid.unique())))\n",
    "print('How many documents are there?: {:,}'.format(len(events_og.document_id.unique())))\n",
    "print('How many displays are there?: {:,}'.format(len(events_og.display_id.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          display_id            uuid  document_id   timestamp platform  \\\n",
      "11635948    11635949  100008d45879bc      1568841   770542274        1   \n",
      "11633862    11633863  100008d45879bc       619529   770408774        1   \n",
      "4511731      4511732  10000a34905274      2108054   296334056        1   \n",
      "502702        502703  10000a34905274      1798241    38774296        1   \n",
      "11706321    11706322  10000a91d9899d      1068414   774838299        2   \n",
      "20422639    20422640  10000a91d9899d      2834573  1160350596        2   \n",
      "7756431      7756432  10000e5327e96b      2258308   537705911        3   \n",
      "4143201      4143202  10000e5327e96b      2066958   264990674        3   \n",
      "12461877    12461878  10000f9ed3fb23      2592689   827245270        2   \n",
      "12466117    12466118  10000f9ed3fb23      2592689   827433223        2   \n",
      "14557958    14557959  10001c9ddb1a9d      2524505   959420538        1   \n",
      "20556500    20556501  10001c9ddb1a9d      1276858  1165612933        1   \n",
      "9338723      9338724  100022acfa4ad3      2108111   635801430        1   \n",
      "13586921    13586922  100022acfa4ad3      2664253   901020502        1   \n",
      "19267665    19267666  100023e47de95c      1255107   899661671        1   \n",
      "4744949      4744950  100023e47de95c      1683555   308698340        1   \n",
      "18289507    18289508  100023e47de95c      1651023   554953569        1   \n",
      "21077478    21077479  1000283c74a0ab      2872696  1187281350        3   \n",
      "13783144    13783145  1000283c74a0ab      2680013   910405327        3   \n",
      "13473969    13473970  1000283c74a0ab      2584944   895632050        3   \n",
      "\n",
      "         geo_location  \n",
      "11635948    US>MA>506  \n",
      "11633862    US>MA>506  \n",
      "4511731     US>OK>650  \n",
      "502702      US>OK>650  \n",
      "11706321    US>IL>602  \n",
      "20422639           US  \n",
      "7756431     US>UT>770  \n",
      "4143201     US>UT>770  \n",
      "12461877    US>CA>803  \n",
      "12466117    US>CA>803  \n",
      "14557958    US>MO>632  \n",
      "20556500    US>MO>632  \n",
      "9338723     US>MA>506  \n",
      "13586921    US>MA>506  \n",
      "19267665    US>NE>652  \n",
      "4744949     US>NE>652  \n",
      "18289507    US>NE>652  \n",
      "21077478    US>CT>533  \n",
      "13783144    US>MA>506  \n",
      "13473969    US>MA>506  \n"
     ]
    }
   ],
   "source": [
    "seen = set()\n",
    "dup = set()\n",
    "for i in events_og.uuid:\n",
    "    if i in seen:\n",
    "        dup.add(i)\n",
    "    seen.add(i)\n",
    "print(events_og[events_og.uuid.isin(dup)].sort_values(by='uuid')[:20])\n",
    "del seen, dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same user can be shown multiple different displays, but this happens rarely and display_id is unique per user_id * document_id. A display_id is associated with multiple documents which we interpret as a document can be pointed to by multiple displays. But since an ad does the actual pointing, an ad can appear in different displays and point to the same document. This is all pretty confusing so here's a diagram to help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'relationship_diagram.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a5be5ccd0bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relationship_diagram.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/sarahkelley/anaconda/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahkelley/anaconda/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahkelley/anaconda/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sarahkelley/anaconda/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'relationship_diagram.png'"
     ]
    }
   ],
   "source": [
    "Image(filename='relationship_diagram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anything interesting about the users in page_views?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             uuid  document_id  timestamp platform geo_location traffic_source\n",
      "0  1fd5f051fba643          120   31905835        1           RS              2\n",
      "1  8557aa9004be3b          120   32053104        1        VN>44              2\n",
      "2  c351b277a358f0          120   54013023        1        KR>12              1\n",
      "3  8205775c5387f9          120   44196592        1        IN>16              2\n",
      "4  9cb0ccd8458371          120   65817371        1    US>CA>807              2\n"
     ]
    }
   ],
   "source": [
    "print(page_views_og.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that clicks are not mentioned here. This indicates page_views is about _any_ page view, not just the ones with a 'success' of a click. Hence the full page_views file is 100GB - there were a lot of documents viewed that never received a click."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we have, it's time to join the data to create a consolidated dataframe that we can use for machine learning. The relationships are complicated due to potential multiple joins and document_id having two different definitions depending on which table it comes from. Therefore, we join tables in a step-wise fashion, checking that the key field per tale is included as we build up our working dataset. Please see inline comments for what happens at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Documents are highest level item in the dataset.\n",
    "\n",
    "# To find the full set of documents covered by our datasets, \n",
    "# pull in the documents that are in both page_views and promoted_content.\n",
    "# We do this because page_views_sample is only a sample of all the page_views (full dataset is 2 billion rows, 100GB)\n",
    "# and if we try to find the page_views that are associated with our clicks_train (in the intuitive order), we won't\n",
    "# be able to find all the page_views because it's only a sample. \n",
    "\n",
    "# Therefore, we 'reverse engineer' our datasets so that we make sure we grab the information that we CAN get from \n",
    "# our sample, mini datasets.\n",
    "\n",
    "doc_ids = set(page_views_og['document_id']) & set(promoted_content_og['document_id'])\n",
    "\n",
    "# To only view displays and ads that are on our 'master' document list,\n",
    "# filter events to documents that are found in page_views and promoted_content.\n",
    "events = events_og[events_og['document_id'].isin(doc_ids)]\n",
    "\n",
    "# We only want to view ad clicks for displays that have document information.\n",
    "# So we filter clicks to displays found in events.\n",
    "clicks_train = clicks_train_og[clicks_train_og['display_id'].isin(events['display_id'])]\n",
    "\n",
    "# Because clicks may not cover all displays shown in events,\n",
    "# re-filter events to display_id's that are found in clicks_train.\n",
    "events = events_og[events_og['display_id'].isin(clicks_train['display_id'])]\n",
    "\n",
    "# By this point we have a solid list of ads, displays, and documents found in our activity files.\n",
    "# Now filter promoted content to ads found in the filtered training dataset for model fitting.\n",
    "promoted_content = promoted_content_og[promoted_content_og['ad_id'].isin(clicks_train['ad_id'])]\n",
    "\n",
    "# We only need document information about documents we are still considering after prior filters.\n",
    "# Filter document content files to only documents that have made it through to promoted_content for space reasons.\n",
    "doc_cats = doc_cats_og[doc_cats_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_ents = doc_ents_og[doc_ents_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_meta = doc_meta_og[doc_meta_og['document_id'].isin(promoted_content['document_id'])]\n",
    "doc_topics = doc_topics_og[doc_topics_og['document_id'].isin(promoted_content['document_id'])]\n",
    "\n",
    "# Similarly, filter page views to documents found in events to make this file manageable.\n",
    "# Since we already filtered based on this file in the beginning, this is re-filtering just to get only the data we need\n",
    "page_views = page_views_og[page_views_og['document_id'].isin(events['document_id'])]\n",
    "\n",
    "# Running into a lot of memory errors, so we try to remove all objects that are unnecessary after use\n",
    "del doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our EDA, we are planning to fit a few models, score them on some basic runs, and then optimize the best performing models. First we will create a large feature space, feature engineer some new variables based on EDA and what features we think will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are predicting on the central data provided through clicks_train, which has a display, the ads in the display, and which ad was clicked. We want to know as much information as possible about the display and the ad, so we start pulling in data from all the other files.\n",
    "\n",
    "First we will pull in information about what display a user saw when they clicked. Events will provide information on what happened at a click event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = clicks_train.merge(events, on='display_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to know more about the ads seen. Remeber: a document can contain multiple displays. In every display, there are multiple ads presented. The document containing the displays will be called document_id_x. Which document the ad is promoting (the document you're taken to when when clicking on the ad) will be document_id_y. Think source (x) and destination (y) documents. All the columns after document_id_y are information about the promoted document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.merge(promoted_content, on='ad_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have meta data about the ads, we need to know more about the promoted document content - what that ad is pointing to. We gather this in the document_X csv's. As noted above, the confidence level deteremines how likely the listed category or topic is actually related to the document.\n",
    "\n",
    "We wanted to duplicate the idea of the CountVectorizer for the 'bag of words' model we used for spam detection, but since we're not counting words in a text, it's a little bit different. We have a 'dictionary' of categories and topics that we use as our 'vocabulary.' Every document has a confidence level for one or more items in the vocabulary. We create a sparse matrix with every topic and category as columns, and every document's associated confidence level value in the respective columns. If the document is not not given a confidence level, we put assume a 0 confidence meaning Outbrain believes the document is unrelated to that category or top.\n",
    "\n",
    "This data on the documents will help us separate ads from one another based on topic/category. For example, why did ad A get clicked instead of ad B? We know ad A referred to document 1 whereas ad B referred to document 2, and now we have general information about the documents the ads referred to. We will merge this information in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# promoted document topics\n",
    "sparsetop = doc_topics.pivot(index='document_id', columns='topic_id', values='confidence_level')\n",
    "sparsetop.columns = ['top_' + str(col) for col in sparsetop.columns]\n",
    "\n",
    "# promoted document categories\n",
    "sparsecat = doc_cats.pivot(index='document_id', columns='category_id', values='confidence_level')\n",
    "sparsecat.columns = ['cat_' + str(col) for col in sparsecat.columns]\n",
    "\n",
    "# join together the topics and categories\n",
    "sparse = sparsetop.join(sparsecat, how='outer')\n",
    "sparse.fillna(0, inplace=True)\n",
    "\n",
    "# remove the separate files to save space but keep the combined file for use with click_teset\n",
    "del sparsetop, sparsecat\n",
    "\n",
    "sparse.reset_index(level=0, inplace=True)\n",
    "\n",
    "# merge in combined category/topics file with data - greatly increases number of columns\n",
    "data = data.merge(sparse, left_on='document_id_y', right_on='document_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training into train and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels length: 282205\n",
      "data length: (282205, 407)\n",
      "\n",
      "training label shape: (197543, 1)\n",
      "training data shape: (197543, 407)\n",
      "dev label shape: (84662, 1)\n",
      "dev data shape: (84662, 407)\n"
     ]
    }
   ],
   "source": [
    "def split_train_dev(dataset, train_percent = 0.7):\n",
    "    '''Splitting data into test and dev.\n",
    "    If train_precent is left at default, 70% will go into training and 30% into development.\n",
    "    If train_percent = 1, all data will go into training.\n",
    "    If train_percent = 0, all data will go into development.'''\n",
    "\n",
    "    labels = data['clicked']\n",
    "    labels = labels.values.reshape(-1,1)\n",
    "\n",
    "    print ('Labels length:', len(labels))\n",
    "    print ('data length:', data.shape)\n",
    "    print ('')\n",
    "        \n",
    "    train_data = data[:int(train_percent*len(data))]\n",
    "    dev_data = data[int(train_percent*len(data)):]\n",
    "\n",
    "    train_labels = labels[:int(train_percent*len(data))]\n",
    "    dev_labels = labels[int(train_percent*len(data)):]\n",
    "\n",
    "    print ('training label shape:', train_labels.shape)\n",
    "    print ('training data shape:', train_data.shape)\n",
    "    print ('dev label shape:', dev_labels.shape)\n",
    "    print ('dev data shape:', dev_data.shape)\n",
    "    \n",
    "    return train_data, train_labels, dev_data, dev_labels\n",
    "\n",
    "train_data, train_labels, dev_data, dev_labels = split_train_dev(data, train_percent = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have three datasets\n",
    "1. train_data, train_labels = 70% of clicks_train dataset. Used for training models.\n",
    "2. dev_data, dev_labels = remaining 30% of clicks_train dataset. Used for measuring accuracy of models on non-train data.\n",
    "3. test_data = clicks_test dataset for final scoring of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know ads are a product of their advertiser, their overall campaign, and the document where they are seen. So we'll add some meta data about how successful the advertiser has been with clicks previously, how successful the campagin has been with clicks previously, and how frequently the document where the ad was seen has been viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# PROCESS ON TRAIN DATA\n",
    "\n",
    "# advertiser success information dictionary -- keep for use with click_teset\n",
    "advr_success = dict(zip(train_data.advertiser_id.unique(),\n",
    "                        [sum(train_data[train_data['advertiser_id']==x]['clicked'])/len(train_data[train_data['advertiser_id']==x]) for x in train_data['advertiser_id'].unique()]))\n",
    "# add advertiser success into dataset\n",
    "train_data['advertiser_perc'] = train_data['advertiser_id'].map(advr_success)\n",
    "\n",
    "# campagin success information dictionary -- keep for use with click_teset\n",
    "camp_success = dict(zip(train_data.campaign_id.unique(), \n",
    "                        [sum(train_data[train_data['campaign_id']==x]['clicked'])/len(train_data[train_data['campaign_id']==x]) for x in train_data['campaign_id'].unique()]))\n",
    "# add campaign success into dataset\n",
    "train_data['campaign_perc'] = train_data['campaign_id'].map(camp_success)\n",
    "\n",
    "# document view frequencies -- keep for use with click_teset\n",
    "doc_view_freq = dict(zip(page_views.document_id.unique(), \n",
    "                         [len(page_views[page_views.document_id==x]) for x in page_views.document_id.unique()]))\n",
    "# add document view frequency into dataset\n",
    "train_data['docx_view_freq'] = train_data['document_id_x'].map(doc_view_freq)\n",
    "\n",
    "\n",
    "# PROCESS ON DEV DATA\n",
    "\n",
    "advr_success = dict(zip(dev_data.advertiser_id.unique(),\n",
    "                        [sum(dev_data[dev_data['advertiser_id']==x]['clicked'])/len(dev_data[dev_data['advertiser_id']==x]) for x in dev_data['advertiser_id'].unique()]))\n",
    "dev_data['advertiser_perc'] = dev_data['advertiser_id'].map(advr_success)\n",
    "camp_success = dict(zip(dev_data.campaign_id.unique(), \n",
    "                        [sum(dev_data[dev_data['campaign_id']==x]['clicked'])/len(dev_data[dev_data['campaign_id']==x]) for x in dev_data['campaign_id'].unique()]))\n",
    "dev_data['campaign_perc'] = dev_data['campaign_id'].map(camp_success)\n",
    "doc_view_freq = dict(zip(page_views.document_id.unique(), \n",
    "                         [len(page_views[page_views.document_id==x]) for x in page_views.document_id.unique()]))\n",
    "dev_data['docx_view_freq'] = dev_data['document_id_x'].map(doc_view_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an ad that has been clicked previously is more likely to be clicked again since many people have similar interests. So we'll conclude our dataset by adding a probability that an ad as been previously clicked given how often it's been clicked. This becomes a little try for an ad that has been viewed infrequently, if ever. To account for this, we add regularization with the following two steps:\n",
    "\n",
    "1. if the ad has not been seen before, assign it the mean probability that any ad will be clicked\n",
    "2. for ads that have been seen, add 1 to their click counts and 10 to their total seen counts\n",
    "\n",
    "These two steps will keep an ad that was seen only once from being a 100% or 0% click success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/nicolelee/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def click_percent(dataset, ad_id, default_result, reg):\n",
    "    '''Returns the posterior probability of ad being clicked.\n",
    "    If ad has not been encountered before, assume mean click'''\n",
    "    \n",
    "    # count number of times ad has been seen\n",
    "    ad_total = len(dataset[dataset['ad_id'] == ad_id])\n",
    "    \n",
    "    # if ad has not been seen, returned the default_results\n",
    "    if ad_total == 0:\n",
    "        return default_result\n",
    "    # otherwise return percentage of times ad has been clicked, adjusted by a regularization term\n",
    "    else:\n",
    "        click_sum = np.sum(dataset[dataset['ad_id'] == ad_id].clicked) + 1.0\n",
    "        return click_sum / (ad_total + reg)\n",
    "\n",
    "# calculate mean times any ad has been clicked\n",
    "mean_click = np.mean(train_data[\"clicked\"])\n",
    "\n",
    "# click posterior frequencies -- keep for use with click_teset\n",
    "click_success_train = dict(zip(train_data.ad_id.unique(), \n",
    "                         [click_percent(train_data, x, mean_click, 10.0) for x in train_data[\"ad_id\"].unique()] ))\n",
    "click_success_dev = dict(zip(dev_data.ad_id.unique(), \n",
    "                         [click_percent(dev_data, x, mean_click, 10.0) for x in dev_data[\"ad_id\"].unique()] ))\n",
    "\n",
    "# add click frequency into dataset\n",
    "train_data['click_perc'] = train_data['ad_id'].map(click_success_train)\n",
    "dev_data['click_perc'] = dev_data['ad_id'].map(click_success_dev)\n",
    "# only based on train data because thats what the train/test will be like\n",
    "\n",
    "del mean_click\n",
    "\n",
    "train_data = train_data.drop('clicked', 1)\n",
    "dev_data = dev_data.drop('clicked', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with the testing dataset (click_test), we have to be careful not to delete any of the original 330M rows (which equates to 6.2M unique display_ids). This has proved challenging throughout development both from an organization standpoint (making sure we don't accidentally filter out rows) and a size standpoint (330M rows is larger than our little laptops can comfortably handle). \n",
    "\n",
    "Further, we discovered, after much upset, that one of the 200ish categories is in the training dataset but not in the testing dataset. This resulted in our final testing dataset having a different number of columns from the training dataset and angered our ML algorithms. To solve for this issue, we have gathered the basic information about the testing data by joining on events and promoted_content (similar to the start of the training dataset). We pull in the other information later (see below under modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull from full events_og to make sure all display information is gathered\n",
    "data_test = clicks_test_og.merge(events_og, on='display_id', how='left')\n",
    "\n",
    "# find the ads from the entire promoted_content_og data and not the one filtered on clicks_train\n",
    "data_test = data_test.merge(promoted_content_og, on='ad_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>display_id</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>uuid</th>\n",
       "      <th>document_id_x</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>platform</th>\n",
       "      <th>geo_location</th>\n",
       "      <th>document_id_y</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>advertiser_id</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_2002</th>\n",
       "      <th>cat_2003</th>\n",
       "      <th>cat_2004</th>\n",
       "      <th>cat_2005</th>\n",
       "      <th>cat_2006</th>\n",
       "      <th>cat_2100</th>\n",
       "      <th>advertiser_perc</th>\n",
       "      <th>campaign_perc</th>\n",
       "      <th>docx_view_freq</th>\n",
       "      <th>click_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197543</th>\n",
       "      <td>12392612</td>\n",
       "      <td>332065</td>\n",
       "      <td>f7bc18373c9d55</td>\n",
       "      <td>1854517</td>\n",
       "      <td>824018058</td>\n",
       "      <td>2</td>\n",
       "      <td>GB&gt;J9</td>\n",
       "      <td>1710793</td>\n",
       "      <td>29309</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099338</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>222</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197544</th>\n",
       "      <td>12392612</td>\n",
       "      <td>377111</td>\n",
       "      <td>f7bc18373c9d55</td>\n",
       "      <td>1854517</td>\n",
       "      <td>824018058</td>\n",
       "      <td>2</td>\n",
       "      <td>GB&gt;J9</td>\n",
       "      <td>1913629</td>\n",
       "      <td>104</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056907</td>\n",
       "      <td>0.056941</td>\n",
       "      <td>222</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197545</th>\n",
       "      <td>12392612</td>\n",
       "      <td>466261</td>\n",
       "      <td>f7bc18373c9d55</td>\n",
       "      <td>1854517</td>\n",
       "      <td>824018058</td>\n",
       "      <td>2</td>\n",
       "      <td>GB&gt;J9</td>\n",
       "      <td>2288852</td>\n",
       "      <td>246</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>222</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197546</th>\n",
       "      <td>12392612</td>\n",
       "      <td>489002</td>\n",
       "      <td>f7bc18373c9d55</td>\n",
       "      <td>1854517</td>\n",
       "      <td>824018058</td>\n",
       "      <td>2</td>\n",
       "      <td>GB&gt;J9</td>\n",
       "      <td>2392456</td>\n",
       "      <td>104</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056907</td>\n",
       "      <td>0.056941</td>\n",
       "      <td>222</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197547</th>\n",
       "      <td>12392620</td>\n",
       "      <td>86365</td>\n",
       "      <td>52990e5aa41541</td>\n",
       "      <td>1649400</td>\n",
       "      <td>824018294</td>\n",
       "      <td>3</td>\n",
       "      <td>GB&gt;H3</td>\n",
       "      <td>1105913</td>\n",
       "      <td>11172</td>\n",
       "      <td>2222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240299</td>\n",
       "      <td>0.189748</td>\n",
       "      <td>9943</td>\n",
       "      <td>0.188948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197548</th>\n",
       "      <td>12392620</td>\n",
       "      <td>105610</td>\n",
       "      <td>52990e5aa41541</td>\n",
       "      <td>1649400</td>\n",
       "      <td>824018294</td>\n",
       "      <td>3</td>\n",
       "      <td>GB&gt;H3</td>\n",
       "      <td>1160717</td>\n",
       "      <td>13651</td>\n",
       "      <td>2555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217597</td>\n",
       "      <td>0.234822</td>\n",
       "      <td>9943</td>\n",
       "      <td>0.230496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197549</th>\n",
       "      <td>12392620</td>\n",
       "      <td>137758</td>\n",
       "      <td>52990e5aa41541</td>\n",
       "      <td>1649400</td>\n",
       "      <td>824018294</td>\n",
       "      <td>3</td>\n",
       "      <td>GB&gt;H3</td>\n",
       "      <td>1313905</td>\n",
       "      <td>17695</td>\n",
       "      <td>2222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240299</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>9943</td>\n",
       "      <td>0.287176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197550</th>\n",
       "      <td>12392620</td>\n",
       "      <td>151353</td>\n",
       "      <td>52990e5aa41541</td>\n",
       "      <td>1649400</td>\n",
       "      <td>824018294</td>\n",
       "      <td>3</td>\n",
       "      <td>GB&gt;H3</td>\n",
       "      <td>1366030</td>\n",
       "      <td>12454</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125340</td>\n",
       "      <td>0.100592</td>\n",
       "      <td>9943</td>\n",
       "      <td>0.102639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197551</th>\n",
       "      <td>12392620</td>\n",
       "      <td>186614</td>\n",
       "      <td>52990e5aa41541</td>\n",
       "      <td>1649400</td>\n",
       "      <td>824018294</td>\n",
       "      <td>3</td>\n",
       "      <td>GB&gt;H3</td>\n",
       "      <td>1464840</td>\n",
       "      <td>21369</td>\n",
       "      <td>3437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>9943</td>\n",
       "      <td>0.007692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197552</th>\n",
       "      <td>12392620</td>\n",
       "      <td>274027</td>\n",
       "      <td>52990e5aa41541</td>\n",
       "      <td>1649400</td>\n",
       "      <td>824018294</td>\n",
       "      <td>3</td>\n",
       "      <td>GB&gt;H3</td>\n",
       "      <td>1659588</td>\n",
       "      <td>16162</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125340</td>\n",
       "      <td>0.130536</td>\n",
       "      <td>9943</td>\n",
       "      <td>0.132692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        display_id   ad_id            uuid  document_id_x  timestamp platform  \\\n",
       "197543    12392612  332065  f7bc18373c9d55        1854517  824018058        2   \n",
       "197544    12392612  377111  f7bc18373c9d55        1854517  824018058        2   \n",
       "197545    12392612  466261  f7bc18373c9d55        1854517  824018058        2   \n",
       "197546    12392612  489002  f7bc18373c9d55        1854517  824018058        2   \n",
       "197547    12392620   86365  52990e5aa41541        1649400  824018294        3   \n",
       "197548    12392620  105610  52990e5aa41541        1649400  824018294        3   \n",
       "197549    12392620  137758  52990e5aa41541        1649400  824018294        3   \n",
       "197550    12392620  151353  52990e5aa41541        1649400  824018294        3   \n",
       "197551    12392620  186614  52990e5aa41541        1649400  824018294        3   \n",
       "197552    12392620  274027  52990e5aa41541        1649400  824018294        3   \n",
       "\n",
       "       geo_location  document_id_y  campaign_id  advertiser_id     ...      \\\n",
       "197543        GB>J9        1710793        29309            210     ...       \n",
       "197544        GB>J9        1913629          104            162     ...       \n",
       "197545        GB>J9        2288852          246            232     ...       \n",
       "197546        GB>J9        2392456          104            162     ...       \n",
       "197547        GB>H3        1105913        11172           2222     ...       \n",
       "197548        GB>H3        1160717        13651           2555     ...       \n",
       "197549        GB>H3        1313905        17695           2222     ...       \n",
       "197550        GB>H3        1366030        12454            185     ...       \n",
       "197551        GB>H3        1464840        21369           3437     ...       \n",
       "197552        GB>H3        1659588        16162            185     ...       \n",
       "\n",
       "        cat_2002  cat_2003  cat_2004  cat_2005  cat_2006  cat_2100  \\\n",
       "197543       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "197544       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "197545       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "197546       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "197547       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "197548       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "197549       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "197550       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "197551       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "197552       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "        advertiser_perc  campaign_perc  docx_view_freq  click_perc  \n",
       "197543         0.099338       0.100000             222    0.100000  \n",
       "197544         0.056907       0.056941             222    0.071429  \n",
       "197545         0.142857       0.000000             222    0.090909  \n",
       "197546         0.056907       0.056941             222    0.090909  \n",
       "197547         0.240299       0.189748            9943    0.188948  \n",
       "197548         0.217597       0.234822            9943    0.230496  \n",
       "197549         0.240299       0.288462            9943    0.287176  \n",
       "197550         0.125340       0.100592            9943    0.102639  \n",
       "197551         0.008197       0.008197            9943    0.007692  \n",
       "197552         0.125340       0.130536            9943    0.132692  \n",
       "\n",
       "[10 rows x 410 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to print any of these datasets to csv, uncomment the appropriate line\n",
    "\n",
    "# data_test.to_csv('testingdata.cvs',index=False)\n",
    "# data.to_csv('fulltrainingdata.cvs',index=False)\n",
    "train_data.to_csv('trainingdata.csv',index=False)\n",
    "train_labels_df= pd.DataFrame(train_labels)\n",
    "train_labels_df.to_csv('traininglabels.csv',index=False)\n",
    "dev_data.to_csv('developmentdata.csv',index=False)\n",
    "dev_labels_df= pd.DataFrame(dev_labels)\n",
    "dev_labels_df.to_csv('developmentlabels.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
