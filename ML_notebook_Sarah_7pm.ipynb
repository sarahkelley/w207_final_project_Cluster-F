{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lr_click_prob = lr.predict_proba(mini_dev[mini_dev.columns[11:]].fillna(0))[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Temporary Data Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "click_train_data= pd.read_csv(\"clicks_train_features.csv\")\n",
    "del click_train_data[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a smaller random sample so that notebook runs easily and grid searches etc don't take so long\n",
    "mini_click =click_train_data.sample(7000, random_state =0)\n",
    "mini_train = mini_click[:5000]\n",
    "mini_dev = mini_click[5000:]\n",
    "mini_train_labels = mini_train[\"clicked\"]\n",
    "mini_dev_labels = mini_dev[\"clicked\"]\n",
    "del mini_train[\"clicked\"]\n",
    "del mini_dev[\"clicked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select only binary and continuous features\n",
    "#eliminate missing data\n",
    "mini_train = mini_train[mini_train.columns[11:]].fillna(0)\n",
    "mini_dev = mini_dev[mini_dev.columns[11:]].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rationale:** \n",
    "\n",
    "**Challenges:** In order to make the data appropriate for regression we had to create binary (one-hot) variables to represent man\n",
    "\n",
    "**Approach:** We first fit a logistic regression model without regularization, using all the binarized features (representing document category, topic etc) and continuous features reprenting the percent of ads from that ad campaign that had been clicked, the percent of ads from that advertiser that had been clicked, the frequency of document views, and the percentage of that ad that had been clicked.\n",
    "\n",
    "Then we use grid search to identify the optimal level of regularization and whether l2 or l1 regularization would perform better. We expected l1 regularization to be superior because it more harshly penalizes very low weighted features. \n",
    "\n",
    "Finally, we tried exercising our domain knowledge/judgement and running a regression with only the four predictors we expected to be most predictive, which were the percent of ads from that ad campaign that had been clicked, the percent of ads from that advertiser that had been clicked, the frequency of document views, and the percentage of that ad that had been clicked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.8455 using all possible features\n",
      "\n",
      "\n",
      "The accuracy is 0.8465 using regularization with the best parameters idenfitied by grid search.\n",
      "These parameters are: l1 penalty and 0.1 C.\n",
      "This leaves 3 non-zero parameters.\n",
      "These features are ['campaign_perc', 'docx_view_freq', 'click_perc']\n",
      "\n",
      "\n",
      "The accuracy is 0.85 using the top 'logically chosen' features.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with all regressable features\n",
    "lr = LogisticRegression()\n",
    "lr.fit(mini_train, mini_train_labels)\n",
    "lr_score_1 = lr.score(mini_dev, mini_dev_labels)\n",
    "print(\"The accuracy is\", lr_score_1, \"using all possible features\")\n",
    "print(\"\\n\")\n",
    "lr = LogisticRegression()\n",
    "C = {\"C\": [.0001, .01, .05, 0.1, .15, .2, .3, .5, 1 ,2, 3, 4, 5, 7, 10, 15, 20, 100, 1000], 'penalty':[\"l1\", \"l2\"]}\n",
    "search = GridSearchCV(lr, param_grid = C)\n",
    "lr_params = search.fit(mini_train, mini_train_labels)\n",
    "lr_best = lr_params.best_params_\n",
    "lr = LogisticRegression(C= lr_best[\"C\"], penalty = lr_best[\"penalty\"])\n",
    "lr.fit(mini_train, mini_train_labels)\n",
    "lr_score_2 = lr.score(mini_dev, mini_dev_labels)\n",
    "print(\"The accuracy is\", lr_score_2, \"using regularization with the best parameters idenfitied by grid search.\")\n",
    "print (\"These parameters are:\", lr_best[\"penalty\"], 'penalty and',lr_best[\"C\"], 'C.')\n",
    "print (\"This leaves\", np.count_nonzero(lr.coef_), \"non-zero parameters.\")\n",
    "features = []\n",
    "for i in range(len(mini_train.columns)):\n",
    "    if lr.coef_[0][i] !=0:\n",
    "        features.append(mini_train.columns[i])\n",
    "print (\"These features are\", features)\n",
    "print(\"\\n\")\n",
    "lr = LogisticRegression()\n",
    "lr.fit(mini_train[mini_train.columns[-4:]], mini_train_labels)\n",
    "lr_score_3 = lr.score(mini_dev[mini_train.columns[-4:]], mini_dev_labels)\n",
    "print(\"The accuracy is\", lr_score_3, \"using the top 'logically chosen' features.\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using the four features chosen for their strongest logical predictive power yields the highest accuracy, although the difference is very small. Regularization choses 3 out of the four chosen \"logically\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rationale:** A random forest algorithm seemed like the perfect technique to approach this since it is a classification problem (ads are either clicked or not clicked) and we had a very large number of features, many of which are likely irrelevant. Random Forest models address decision trees issues with over fitting by averaging the results of many different decision trees trained on different sub samples of the data. This increases variance but decreases bias, leading to more accurate prediction on test data. \n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "**Approach:** We trained first a standard decision tree, and then trained a random forest model using grid search to optimize the maximum number of features the trees would use, the number of trees, and the information gain criterion (either gini or entropy). We then scored the model using the best features. \n",
    "We also then fit an extra trees model, which tends towards a larger number of leaves, using the same grid search parameters as with the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single decision tree got an accuracy of 0.81\n"
     ]
    }
   ],
   "source": [
    "#this may be overfit so not good in the longterm\n",
    "dec_tree = tree.DecisionTreeClassifier()\n",
    "dec_tree = dec_tree.fit(mini_train, mini_train_labels)\n",
    "print(\"A single decision tree got an accuracy of\", dec_tree.score(mini_dev, mini_dev_labels))\n",
    "parameters = {'n_estimators':[50, 100, 400, 500], 'criterion': [\"gini\", \"entropy\"]}\n",
    "forest = RandomForestClassifier(random_state = 1)\n",
    "search = GridSearchCV(forest, param_grid = parameters, scoring = \"f1_micro\")\n",
    "forest_params = search.fit(mini_train, mini_train_labels)\n",
    "forest_best = forest_params.best_params_\n",
    "forest = RandomForestClassifier(random_state = 1, max_features = auto, n_estimators = forest_best['n_estimators'], criterion = forest_best['criterion'])\n",
    "forest.fit(mini_train, mini_train_labels)\n",
    "forest.score(mini_dev, mini_dev_labels)\n",
    "print(\"A random forest with optimal parameters got an accuracy of\", forest.score(mini_dev, mini_dev_labels))\n",
    "print(\"With\", forest_best['n_estimators'], 'trees and', forest_best['criterion'], \"as the criterion\")\n",
    "parameters = {'n_estimators':[50, 100, 400, 500], 'criterion': [\"gini\", \"entropy\"]}\n",
    "extrees = ExtraTreesClassifier(random_state = 1)\n",
    "search = GridSearchCV(extrees, param_grid = parameters, scoring = \"f1_micro\")\n",
    "extrees_params = search.fit(mini_train, mini_train_labels)\n",
    "extrees_best = extrees_params.best_params_\n",
    "trees = ExtraTreesClassifier(random_state = 1, max_features = 'auto', n_estimators = forest_best['n_estimators'], criterion = forest_best['criterion'])\n",
    "extrees.fit(mini_train, mini_train_labels)\n",
    "extrees.score(mini_dev, mini_dev_labels)\n",
    "print(\"A random forest with optimal parameters got an accuracy of\", extrees.score(mini_dev, mini_dev_labels))\n",
    "print(\"With\", extrees_best['n_estimators'], 'trees, ', extrees_best['criterion'], \"as the criterion, and\", extrees_best['max_features'], \"as the maximum number of features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarah's scratch working, don't delete yet but don't run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81000000000000005"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tree = tree.DecisionTreeClassifier()\n",
    "#Baseline model with just the \"fabulous four\" predictors\n",
    "dec_tree = dec_tree.fit(mini_train[mini_train.columns[-4:]].fillna(0), mini_train_labels)\n",
    "dec_tree.score(mini_dev[mini_dev.columns[-4:].fillna(0)], mini_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 400, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "#RandomForest works well\n",
    "parameters = {'n_estimators':[10, 30, 40, 100, 400, 500, 700, 1000], 'criterion': [\"gini\", \"entropy\"]}\n",
    "forest = RandomForestClassifier(random_state = 1)\n",
    "search = GridSearchCV(forest, param_grid = parameters, scoring = \"f1_micro\" )\n",
    "forest_params = search.fit(mini_train[mini_train.columns[-4:]].fillna(0), mini_train_labels)\n",
    "forest_best = forest_params.best_params_\n",
    "print(forest_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82099999999999995"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(random_state = 0, n_estimators = forest_best['n_estimators'], criterion = forest_best['criterion'])\n",
    "forest.fit(mini_train, mini_train_labels)\n",
    "forest.score(mini_dev, mini_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 40, 'max_features': 200, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "#RandomForest works well\n",
    "parameters = {'n_estimators':[5, 10, 15, 20, 25, 30, 40], 'criterion': [\"gini\", \"entropy\"], 'max_features': [1, 3, 5, 10, 20, 40, 100, 200, 300]}\n",
    "forest = RandomForestClassifier(random_state = 1)\n",
    "search = GridSearchCV(forest, param_grid = parameters, scoring = \"f1_micro\" )\n",
    "forest_params = search.fit(mini_train[mini_train.columns[11:]].fillna(0), mini_train_labels)\n",
    "forest_best = forest_params.best_params_\n",
    "print(forest_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82250000000000001"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(random_state = 1, n_estimators = 400, criterion = 'gini')\n",
    "forest.fit(mini_train[mini_train.columns[11:]].fillna(0), mini_train_labels)\n",
    "forest.score(mini_dev[mini_dev.columns[11:]].fillna(0), mini_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 20, 'criterion': 'entropy'}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'n_estimators':[5, 10, 15, 20, 25, 30, 40], 'criterion': [\"gini\", \"entropy\"]}\n",
    "trees = ExtraTreesClassifier(random_state = 1)\n",
    "search = GridSearchCV(trees, param_grid = parameters, scoring = \"f1_micro\" )\n",
    "trees_params = search.fit(mini_train[mini_train.columns[-4:]].fillna(0), mini_train_labels)\n",
    "trees_best = trees_params.best_params_\n",
    "print(trees_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82399999999999995"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtratrees = ExtraTreesClassifier(n_estimators =20, criterion = 'entropy')\n",
    "xtratrees = xtratrees.fit(mini_train[mini_train.columns[-4:]].fillna(0), mini_train_labels)\n",
    "xtratrees.score(mini_dev[mini_dev.columns[-4:].fillna(0)], mini_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(mini_train)\n",
    "proj = pca.transform(mini_train)\n",
    "cmap = LinearSegmentedColormap.from_list('mycmap', [(0, \"red\"), (1, \"blue\")])\n",
    "plt.scatter(proj[:,0], proj[:,1], c = mini_train_labels, cmap=cmap)\n",
    "plt.title(\"Clicks Data Projected Two Dimensions\")\n",
    "plt.show()\n",
    "\n",
    "'''pca = PCA(n_components = i)\n",
    "pca.fit(mini_train[mini_train.columns[11:]].fillna(0))\n",
    "pca\n",
    "for i in range(50):\n",
    "    print(\"With\", i, \"components, the explained variance is\", sum(pca.explained_variance_ratio_[:i+1]))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>display_id</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>uuid</th>\n",
       "      <th>document_id_x</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>platform</th>\n",
       "      <th>geo_location</th>\n",
       "      <th>document_id_y</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_2002</th>\n",
       "      <th>cat_2003</th>\n",
       "      <th>cat_2004</th>\n",
       "      <th>cat_2005</th>\n",
       "      <th>cat_2006</th>\n",
       "      <th>cat_2100</th>\n",
       "      <th>campaign_perc</th>\n",
       "      <th>advertiser_perc</th>\n",
       "      <th>docx_view_freq</th>\n",
       "      <th>click_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>210701</th>\n",
       "      <td>210701</td>\n",
       "      <td>12970337</td>\n",
       "      <td>174546</td>\n",
       "      <td>86559a64294b03</td>\n",
       "      <td>1449981</td>\n",
       "      <td>854783252</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;IL&gt;609</td>\n",
       "      <td>1439845</td>\n",
       "      <td>21331</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029557</td>\n",
       "      <td>0.018605</td>\n",
       "      <td>6680</td>\n",
       "      <td>0.033708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255507</th>\n",
       "      <td>255507</td>\n",
       "      <td>15329563</td>\n",
       "      <td>159253</td>\n",
       "      <td>7752ddb03b7fa5</td>\n",
       "      <td>1331987</td>\n",
       "      <td>1015941667</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ&gt;E7</td>\n",
       "      <td>1393979</td>\n",
       "      <td>20101</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.120755</td>\n",
       "      <td>10</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138897</th>\n",
       "      <td>138897</td>\n",
       "      <td>9087365</td>\n",
       "      <td>51402</td>\n",
       "      <td>f813ccbbac2938</td>\n",
       "      <td>471778</td>\n",
       "      <td>611374801</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;ND&gt;687</td>\n",
       "      <td>973567</td>\n",
       "      <td>6345</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200876</td>\n",
       "      <td>0.206905</td>\n",
       "      <td>1537</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206435</th>\n",
       "      <td>206435</td>\n",
       "      <td>12781117</td>\n",
       "      <td>497811</td>\n",
       "      <td>81a107d04e9107</td>\n",
       "      <td>1362397</td>\n",
       "      <td>843154067</td>\n",
       "      <td>1</td>\n",
       "      <td>US&gt;CA&gt;807</td>\n",
       "      <td>1349628</td>\n",
       "      <td>32853</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088207</td>\n",
       "      <td>5423</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121461</th>\n",
       "      <td>121461</td>\n",
       "      <td>7856231</td>\n",
       "      <td>143662</td>\n",
       "      <td>244cabf7ce72a2</td>\n",
       "      <td>1291818</td>\n",
       "      <td>546456480</td>\n",
       "      <td>2</td>\n",
       "      <td>US&gt;NJ&gt;501</td>\n",
       "      <td>1085917</td>\n",
       "      <td>18390</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.331646</td>\n",
       "      <td>0.316629</td>\n",
       "      <td>10751</td>\n",
       "      <td>0.325926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  display_id   ad_id            uuid  document_id_x  \\\n",
       "210701      210701    12970337  174546  86559a64294b03        1449981   \n",
       "255507      255507    15329563  159253  7752ddb03b7fa5        1331987   \n",
       "138897      138897     9087365   51402  f813ccbbac2938         471778   \n",
       "206435      206435    12781117  497811  81a107d04e9107        1362397   \n",
       "121461      121461     7856231  143662  244cabf7ce72a2        1291818   \n",
       "\n",
       "         timestamp  platform geo_location  document_id_y  campaign_id  \\\n",
       "210701   854783252         2    US>IL>609        1439845        21331   \n",
       "255507  1015941667         3        NZ>E7        1393979        20101   \n",
       "138897   611374801         2    US>ND>687         973567         6345   \n",
       "206435   843154067         1    US>CA>807        1349628        32853   \n",
       "121461   546456480         2    US>NJ>501        1085917        18390   \n",
       "\n",
       "           ...      cat_2002  cat_2003  cat_2004  cat_2005  cat_2006  \\\n",
       "210701     ...             0         0         0         0         0   \n",
       "255507     ...             0         0         0         0         0   \n",
       "138897     ...             0         0         0         0         0   \n",
       "206435     ...             0         0         0         0         0   \n",
       "121461     ...             0         0         0         0         0   \n",
       "\n",
       "        cat_2100  campaign_perc  advertiser_perc  docx_view_freq  click_perc  \n",
       "210701         0       0.029557         0.018605            6680    0.033708  \n",
       "255507         0       0.315789         0.120755              10    0.250000  \n",
       "138897         0       0.200876         0.206905            1537    0.282000  \n",
       "206435         0       0.000000         0.088207            5423    0.090909  \n",
       "121461         0       0.331646         0.316629           10751    0.325926  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['campaign_perc', 'advertiser_perc', 'docx_view_freq', 'click_perc'], dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_train.columns[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pydotplus \\ndot_data = tree.export_graphviz(dec_tree, out_file=None) \\ngraph = pydotplus.graph_from_dot_data(dot_data) \\ngraph.write_pdf(\"dec_tree.pdf\") '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pydotplus \n",
    "dot_data = tree.export_graphviz(dec_tree, out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "graph.write_pdf(\"dec_tree.pdf\") '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from IPython.display import Image  \\ndot_data = tree.export_graphviz(dec_tree, out_file=None, \\n                     feature_names=[\"% of Campaign Clicked\", \"% of Advertizer Clicked\", \"Document View Frequency\", \"% of Ad Clicked\"],  \\n                     class_names=[\"Not Clicked\", \"Clicked\"],  \\n                     filled=True, rounded=True,  \\n                     special_characters=True)  \\ngraph = pydotplus.graph_from_dot_data(dot_data)  \\nImage(graph.create_png())  \\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from IPython.display import Image  \n",
    "dot_data = tree.export_graphviz(dec_tree, out_file=None, \n",
    "                     feature_names=[\"% of Campaign Clicked\", \"% of Advertizer Clicked\", \"Document View Frequency\", \"% of Ad Clicked\"],  \n",
    "                     class_names=[\"Not Clicked\", \"Clicked\"],  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#odd problem when trying to use all the data[data[11:]]\n",
    "#ValueError: Input contains NaN, infinity or a value too large for dtype('float32')\n",
    "#not nan or infinite\n",
    "test = mini_train[mini_train.columns[11:]].fillna(0)\n",
    "test.isnull().values.any()\n",
    "np.any(np.isnan(test))\n",
    "np.all(np.isfinite(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80449999999999999"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb = nb.fit(mini_train[mini_train.columns[-4:]], mini_train_labels)\n",
    "nb_preds = nb.predict(mini_dev[mini_dev.columns[-4:]])\n",
    "nb_score = nb.score(mini_dev[mini_dev.columns[-4:]], mini_dev_labels)\n",
    "#metrics.f1_score(mini_dev_labels, preds , average = \"micro\")\n",
    "nb_score\n",
    "\n",
    "knn = KNN()\n",
    "knn = knn.fit(mini_train[mini_train.columns[-4:]].fillna(0), mini_train_labels)\n",
    "knn_preds = knn.predict(mini_dev[mini_dev.columns[-4:]].fillna(0))\n",
    "knn_score = knn.score(mini_dev[mini_dev.columns[-4:]].fillna(0), mini_dev_labels)\n",
    "#metrics.f1_score(mini_dev_labels, preds , average = \"micro\")\n",
    "knn_score"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
